---
title: "Matt's Analysis"
author: "Matt Crump"
date: "6/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```


# Mean IKSIs as a function of letter position and word length

First get individual subject means, then plot grand means averaging over individual subjects.

The figure shows lots of evidence for the **first letter slowing** phenomena, where the first letter in a word is typed more slowly than the remaining letters. This has been interpreted in terms of planning time. People buffer an entire word as a part of planning to type the word. This time is reflected in the first keystroke, after they start typing, the remaining keystrokes are faster because they are more automatically rattling of the planned letter sequence.

There is also evidence for **mid-word slowing**, where the IKSIs (interkeystroke intervals) are longer in the middle positions of the word compared to the earlier (excluding first position) and later positions. Not much attention has been paid to the explanation of mid-word slowing. Could also be interpreted in terms of the same buffering/planning process that produces first letter slowing. For example, people might also plan at the level of syllables, which are often in the middle of words. Mid-word slowing could reflect syllable-level buffering, which would typically slow down production of letters in the middle of a word. 

**Some of the longer words have longer IKSIs at the end, should double-check to make sure coding of the data is not weird here**

In the next analysis, we will explore a different interpretation of these slowing phenomena in terms of letter predictability. 

```{r}
library(data.table)
library(dplyr)
library(ggplot2)
library(Crump) #for standard error function and Van Selst and Jolicouer outlier elimination

# mturk.txt is the unzipped mturk.txt.zip file
the_data <- fread("~/Desktop/mturk.txt")

################
# Data-Exclusion
# remove really long IKSIs
the_data <- the_data[the_data$IKSIs < 2000,]

# remove incorrect keystrokes, and keystrokes where n-1 was incorrect (remove post-error slowing)
the_data <- the_data[the_data$PredBigramCorrect=="11",]

# restrict to normal word paragraphs
the_data <- the_data[the_data$ParagraphType=="N",]

###############
# Analysis
# Get the means by word length and letter position for each subject
subject_means <- the_data %>%
              group_by(Subject,word_lengths,let_pos) %>%
              summarize(mean_IKSI = mean(IKSIs, na.rm = TRUE))

# Get the grand means by averaging over subject means
sum_data <- subject_means %>%
              group_by(word_lengths,let_pos) %>%
              summarize(mean_IKSIs = mean(mean_IKSI, na.rm = TRUE),
                        SE = stde(mean_IKSI))

# plot the data

sum_data <- sum_data[sum_data$let_pos < 10, ]
sum_data <- sum_data[sum_data$word_lengths < 10 &
                     sum_data$word_lengths > 0, ]

sum_data$let_pos<-as.factor(sum_data$let_pos)
sum_data$word_lengths<-as.factor(sum_data$word_lengths)

limits <- aes(ymax = mean_IKSIs + SE, ymin = mean_IKSIs - SE)

ggplot(sum_data,aes(x=let_pos,y=mean_IKSIs,group=word_lengths,color=word_lengths))+
  geom_line()+
  geom_point()+
  geom_errorbar(limits,width=.2)+
  theme_classic()+
  ggtitle("Mean IKSI as a Function of Letter Position and Word Length")

```

# Letter uncertainty (H) as a function of position and word length

1. Use information theory to calculate the amount of entropy in letter distributions {$H = -\sum p \log_2 p$)
2. Maximum entropy for random occurences of letters (assuming 26) is 4.7004. {$H = -\sum \frac{1}{26} \log_2 \frac{1}{26} = 4.7004$). Maximum entropy occurs when each letter occurs equally frequently, which we will take as our definition of randomness, or complete lack of structure.
3. Letters occur in english writing in a non-random fashion. Some letters are more frequent than others, as a result they are more predictable and less random.
4. We can calculate H for any frequency distribution of letters by turning the frequencies into probabilities and applying Shannon's formula. H will range between 0 and it's maximum value 4.7004. As the predictability of the letters increases, H will tend toward 0. As the predictability decreases, H will tend toward 4.7004. Or, in opposite: As the entropy increases, H goes to 4.7004; as the entrop decreases H goes to 0. To use information-theory speak, we could also say that as the information in the set increases, H goes to 4.7004, and as the information in the set decreases, H goes to 0. "Information" here is defined in terms of entropy. This may seem a bit odd, because the most random situation is the one that gives the most information. The idea is that in a situation where events are predictable, each upcoming event may not give the observer much new information. If the observer can predict the event, then the observer doesn't learn anything new from the occurence of the event (which is why predictable situations have low information).
5. Now that we know that we can calculate the amount of entropy in a letter distribution, we can ask whether the entropy varies across different kinds of letter distributions. For example, if we were to determine the letter frequency distribution for all 1-letter words that have occured in written english we could calculate an H for that. What about the letter frequency distribtuion for all letters occuring in position 1 of four letter words? or position 5 of 7 letter words. Or how about all of the distributions for all of the letters occuring in all of the positions of all of the words of different lengths? 
6. Lucky for us, Peter Norvig from Google wrote a blog where he did some of this counting for us, based off of google's digitized book project. He provided a spreadsheet that counts the occurences of letters in all of the letter positions from 1-9 of words of length 1-9. 
7. As a result, we can simply calculate H for all of those letter distributions.

## Our question?

1. Does the variation in H across letter position and word length explain variance in mean typing speed for letters in those positions across word length. If so, perhaps the first letter and mid-word slowing effects reflect constraints on performance driven by a process sensitive to the predictability of individual letters.

2. Does H vary at all across letter positions and word lengths? Because I already ran this analysis two years ago I know that it does, so let's double-check and do it again. I don't know if this has been reported anywhere, and haven't looked terribly hard to determine if it has beyond Norvig's blogging of the frequency tables.

```{r}
library(bit64)
# load in the excel file from Norvig:
letter_freqs <- fread("ngrams1.csv",integer64="numeric")
letter_freqs[letter_freqs==0]<-1

letter_probabilities <- apply(letter_freqs[,2:74],2,function(x){x/sum(x)})

letter_entropies <- apply(letter_probabilities,2,function(x){-1*sum(x*log2(x))})

position<-c(1,1:2,1:3,1:4,1:5,1:6,1:7,1:8,1:9)
word_length<-c(1,rep(2,2),
               rep(3,3),
               rep(4,4),
               rep(5,5),
               rep(6,6),
               rep(7,7),
               rep(8,8),
               rep(9,9))

uncertainty_df<-data.frame(H=letter_entropies[11:(11+44)],position,word_length)

#plot

ggplot(uncertainty_df,aes(x=position,y=H,group=word_length,color=word_length))+
  geom_line()+
  geom_point()+
  theme_classic()+
  ggtitle("Mean Entropy (H) as a Function of Letter Position and Word Length")

```

# plot Mean IKSI as a function of H for different letter positions and word lengths

There is a general trend for mean letter IKSI to increase as a function of H. Not as neat and tidy as the Hick-Hyman law.

```{r}

sum_data<-cbind(sum_data,H=uncertainty_df$H)

#plot

ggplot(sum_data,aes(x=H,y=mean_IKSIs, color=let_pos))+
  geom_point()+
  theme_classic()+
  ggtitle("Mean IKSI as a function of Entropy")

```

## facet_wrapping by word_lengths

```{r}

#plot

ggplot(sum_data,aes(x=H,y=mean_IKSIs))+
  geom_point()+
  theme_classic()+
  ggtitle("Mean IKSI as a function of Entropy, split by word_lengths")+
  facet_wrap(~word_lengths)

```

## facet_wrapping by position

Linear relationship here seems somewhat more apparent, at least where there is enough data points to look for it.

```{r}

#plot

ggplot(sum_data,aes(x=H,y=mean_IKSIs))+
  geom_point()+
  theme_classic()+
  ggtitle("Mean IKSI as a function of Entropy, split by position")+
  facet_wrap(~let_pos)

```



