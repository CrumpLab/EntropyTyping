<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Instance theory predicts information theory: Episodic uncertainty as a determinant of keystroke dynamics</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>


<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>

<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="webpaper.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Entropy Typing</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Entropy_typing_draft.html">Paper</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Instance theory predicts information theory: Episodic uncertainty as a determinant of keystroke dynamics</h1>
<h4 class="author"><em>Matthew J. C. Crump</em></h4>
<address class="author_afil">
1<br><a class="author_email" href="mailto:#"><a href="mailto:mcrump@brooklyn.cuny.edu">mcrump@brooklyn.cuny.edu</a></a>
</address>
<h4 class="author"><em>Walter Lai</em></h4>
<address class="author_afil">
1<br><h4 class="author"><em>Nicholaus Brosowsky</em></h4>
<address class="author_afil">
1<br><div class="abstract">
<p class="abstract">Abstract</p>
<p>How does prior experience shape skilled performance in structured environments? We use skilled typing of natural text to evaluate correspondence between performance (keystroke timing) and structure in the environment (letter uncertainty). We had ~350 typists copy-type english text. We reproduced Ostry’s (1983) analysis of interkeystroke interval as a function of letter position and word length, which showed prominent first-letter and mid-word slowing effects. We propose a novel account that letter position and word length effects on keystroke dynamics reflect informational uncertainty about letters in those locations, rather than resource limited planning/buffering processes. We computed positional uncertainty for letters in all positions of words from length one to nine using Google’s n-gram database. We show that variance in inter-keystroke interval by letter position and word length tracks natural variation in letter uncertainty. Finally, we provide a model showing how a general learning and memory process could acquire sensitivity to patterns of letter uncertainty in natural english. In doing so, we draw an equivalence between Logan’s (1988) instance theory of automatization and Shannon’s measure of entropy (H) from information theory. Instance theory’s predictions for automatization as a function of experience follow exactly the uncertainty in the choice set being automatized. As a result, instance theory stands as a general process model explaining how context-specific experiences in a structured environment tune skilled performance.</p>
</div>

</div>


<pre class="r"><code># Seed for random number generation
set.seed(42)</code></pre>
<!-- Big issues: how people learn about the structure of their environment
   - need to have a way to measure the structure in the environment (information theory)
   - need to have a theory/model about how structure in the enviroment is learned (instance theory
   - need to have laboratory/real-world environment where behavior in interaction with environment can be measured (typing) -->
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Theories of cognitive processes run along a continuum. On one extreme, cognitive phenomena are explained in terms specialized and dedicated modules <span class="citation">(Fodor 1983)</span> that give rise to cognition by the principles of their internal processing architecture. On the other extreme, cognitive phenomena are explained in terms of general learning and memory processes <span class="citation">(Jacoby and Brooks 1984; Kolers and Roediger 1984; Rumelhart and McClelland 1986)</span> that give rise to cognition through their experience with a structured environment <span class="citation">(Clark 2008)</span>. Valid theories produce explanations of phenomena by deduction from their processing assumptions, and then compete with other valid theories on the basis of parsimony. When a phenomena is explained by a general process, specialized accounts become sufficient, but not necessary; and, vice versa. We continue in this tradition by proposing and validating a general process account of keystroke dynamics in skilled typing performance. We show that keystroke dynamics can emerge from a general memory process sensitive to structure (uncertainty) in the natural language environment.</p>
<!-- Current aims and broader implications of what we are doing beyond typing domain
   - What we are trying to do here, using typing (which measures learning about a structured natural language environment) as a laboratory tool to develop explanatory models of how people learn from the structure of the environment
   - brief foreshadowing of how this can be achieved (e.g., by focusing on phenomena such as first-letter and mid-word slowing, and thinking about whether this phenomena emerges naturally from learning about the structure of the typing environment, or requires special process explanations) -->
<p>We identified the following pre-requisites as necessary for our approach. We follow the assumption that specialized or general processes of cognition are constrained to operate upon the structure of their environmental inputs. So, we require a tool for describing the structure of environmental inputs. We assume that performance is driven by learning processes sensitive to the structure in the environment. So, we require a model that articulates how learning about the structure of an environment produces performance. Finally, we require a task where the relation between performance and a structured environment can be measured. We use information theory <span class="citation">(Shannon and Weaver 1998)</span> to measure the structure of the letters that typists’ type, instance theory <span class="citation">(Logan 1988)</span> to model how typists’ performance is shaped by the typing environment, and the task of continuous typing <span class="citation">(Logan and Crump 2011)</span> to measure keystroke dynamics as a function of the structure in the typing environment.</p>
<p>There are many typing phenomena to explain <span class="citation">(Salthouse 1986)</span>, and several existing models of typing <span class="citation">(Heath and Willcox 1990; John 1996; Rumelhart and Norman 1982; Wu and Liu 2008)</span>. Our goal here was not to provide another general model of typing, and we expect that our model will fail to explain many aspects of typing performance. Instead, we focus our efforts empirically and theoretically as follows. Empirically, we examine whether typing performance is constrained by structure in the natural language. Theoretically, we propose a general processing account that predicts how structure in the natural language should constrain typing performance. These aims contribute to the broader goals (beyond the scope of this paper) of determining whether specialized or general accounts are neccessary or sufficient to explain typing performance, and then adjucating between them.</p>
<!-- Typing phenomena (first-letter and mid-word slowing)
   - introduce each (Ostry, 1983)
-->
<p>We focused on two typing phenomena, the word-initiation/first-letter slowing effect, and the mid-word slowing effect, which are both observed in continuous copy-typing of words presented in sentences. First-letter slowing refers to longer keystroke times for letters in the first position of a word compared to other letters. Mid-word slowing refers to an inverted U shaped pattern, with longer interkeystroke intervals for letters in the middle of a word compared to letters at the beginning and ending of a word. First-letter and mid-word slowing were clearly demonstrated by <span class="citation">Ostry (1983)</span>, who showed systematic effects of letter position and word length on interkeystroke intervals.</p>
<p>We chose these phenomena for two reasons. First, both phenomena have been explained in terms of specialized processes, and it remains unclear whether those accounts are necessary to explain the phenomena. Second, we have not found work replicating Ostry’s (1983) results, and <span class="citation">Salthouse (1986)</span> suggested that effects of word length do not systematically influence interkeystroke intervals, so the effects of letter position and word length on interkeystroke interval remain unclear.</p>
<!-- Existing explanations of typing phenomena
   - brief review of existing explanations. These are likely to be special process explanations (e.g., a word-buffereing planning process)
-->
<p>First-letter slowing has been explained in terms of planning and buffering processes associated with typing a whole word. For example, the time associated with retrieving a motor program for a word, parsing the word into letters, planning the sequence, or initiating the execution of the sequence after it is buffered, could cause the first letter in a word to be produced more slowly than other letters. Mid-word slowing has been explained in terms of rising interference from ongoing sequencing, or from micro-planning of syllables which often occur in the middle of words <span class="citation">(Will, Nottbusch, and Weingarten 2006)</span>. These explanations rely on largely unspecified planning and execution processes that are reverse-engineered by imputing hypotheses about their operation from typing data.</p>
<!--Propose a more general learning and memory process explanation
   - Establish plausibility by describing prior evidence supporting the idea that general learning and memory processes are involved in typing (e.g., prior evidence that typists are sensitive to frequency effects).
   - Describe how the first-letter and mid-word slowing phenomena could emerge from a process sensitive to letter uncertainty
   - Describe what would be required for this to work...e.g., letter uncertainty would have to roughly follow the pattern of first-letter and mid-word slowing
   - Describe that a model would be needed to explain how a general learning and memory process could learn from experience, and produce performance that was constrained by letter uncertainty (ie., the structure in the natural language).
-->
<p>To develop an alternative, we entertained a simple question: are more predictable letters typed faster than less predictable letters? More specifically, we wondered whether natural variation in letter uncertainty as a function of letter position and word length would magically <span class="citation">(in the sense of Miller 1956)</span> correspond to the observed variation in interkeystroke intervals as a function of letter position and word length. Such a demonstration would license consideration of how a general learning process sensitive to letter uncertainty could explain effects of letter position and word length on interkeystroke intervals.</p>
<p>Prior work shows that typists are sensitive to structures in the text the type. For example, IKSIs are correlated with letter and bigram frequency <span class="citation">(Behmer and Crump 2017; Grudin and Larochelle 1982; Salthouse 1984; Terzuolo and Viviani 1980)</span>, trigram frequency <span class="citation">(Behmer and Crump 2017)</span>, and word frequency <span class="citation">(Vinson 2017)</span>. Individual keystroke times are influenced by the immediate letter context in which they occur <span class="citation">(Shaffer 1973; Gentner 1982)</span>. IKSIs are also influenced by orthographic structure <span class="citation">(Massaro and Lucas 1984; Pinet, Ziegler, and Alario 2016; Will, Nottbusch, and Weingarten 2006)</span>. Finally, IKSIs are much faster for letter strings from a natural language, compared to random letter strings <span class="citation">(Shaffer and Hardwick 1968)</span>; <span class="citation">Behmer and Crump (2017)</span>]. These demonstrations suggest that typing performance is partly determined by a learning process sensitive to structure inherent to natural texts.</p>
<!-- Information theory and reaction time
   - explain what entropy means, how to calculate it
   - brief history of importance in reaction time research
   - end with where we at, and how it can be useful
-->
<p>Following <span class="citation">Shannon and Weaver (1998)</span>, we use information theory as a tool to measure structure in natural texts. The summary statistic H measures the entropy or uncertainty in any discrete probability distribution of a set of items. H goes to 0 for distributions that are perfectly predictable (e.g., when one item occurs 100% of the time). H goes to it’s maximum value for distributions that are completely unpredictable, fully entropic, or maximally uncertain (e.g., when all items occur with equal probability). Shannon’s H is defined as:</p>
<p><span class="math inline">\(H = -\sum p \log_2 p\)</span></p>
<p>where, p is the probability of occurence for each item in a given distribution. H is the number of bits needed to represent the distribution. To apply this to letter uncertainty, consider the set of the 26 lowercase letters from a to z. For this set, H can range from 0 to ~4.7. H approaches 4.7 as letter probabilities approach a uniform distribution, indicating all letters are equiprobable, <span class="math inline">\(H = -\sum \frac{1}{26} \log_2 \frac{1}{26} = 4.7004\)</span>. H by definition is less than 4.7 for all unequal letter probability distributions, where some letters occur with higher/lower probabilities than others.</p>
<p>Most important, H can be calculated for any letter probability distribution. For example, if separate letter probability distributions for every letter position across words of every length in natural English text could be obtained, then the letter uncertainty for each position by word length could be calculated; and, correspondence between letter uncertainty and interkeystroke intervals as a function of letter position and word length could be evaluated.</p>
<p>Our empirical question also ties into the well known application of information theory to choice-reaction time performance. For example, <span class="citation">Hick (1952)</span>, and <span class="citation">Hyman (1953)</span> showed that choice reaction time, which was known to increase as a function of set-size, increases linearly as a function of choice uncertainty in the set (measured by H), rather than set-size persay. Although there are numerous exceptions to the Hick-Hyman law <span class="citation">(for a review see, Proctor and Schneider 2018)</span>, we are not aware of any work that has determined whether typing performance (a continuous 26-AFC choice-task, assuming lower case for convenience) depends on letter uncertainty. If typing performance does depend on letter uncertainty, then a model based explanation of the dependency is required.</p>
<!-- Roadmap what will we do in the paper
   - Part 1: Show that we can replicate the phenomena (Experiment 1)
   - Part 2: Show that we can measure letter uncertainty, and that it does explain some of the variance in the behavioral data
   - Part 3: Show that we can model the relationship between mean_iksi and letter uncertainty with an instance theory
   - Part 4: Discuss
-->
<div id="overview-of-present-study" class="section level2">
<h2>Overview of present study</h2>
<p>We first reproduce Ostry’s (1983) analysis of interkeystroke intervals as a function of letter position and word length. We used the dataset collected by <span class="citation">Behmer and Crump (2017)</span>, who had 346 typists copy type five paragraphs of natural english text. Then we estimated letter uncertainty in natural english for each letter position in words of different lengths. We used letter frequency counts from Google’s Ngram project provided by Peter Norvig, which gave us letter uncertainty estimates for each position in words of length one to nine. Next, we show that natural variation in letter uncertainty can explain large portions of variance in interkeystroke intervals as a function of letter position and word length. Finally, we show that the instance theory of automatization <span class="citation">(Logan 1988)</span> provides a working process model explaining how a general memory process could cause typing performance to be constrained by letter uncertainty.</p>
</div>
</div>
<div id="methods" class="section level1">
<h1>Methods</h1>
<div id="participants" class="section level2">
<h2>Participants</h2>
<p>400 participants were recruited from Amazon’s mechanical turk (restricted to people from the USA, with over 90% completion rate). Data were only analyzed for the 346 participants who successfully completed the task (98 men, 237 women, 11 no response). Additional demographic information is reported in <span class="citation">Behmer and Crump (2017)</span>. The procedure was approved by the institutional review board at Brooklyn College of the City University of New York.</p>
</div>
<div id="stimuli-and-apparatus" class="section level2">
<h2>Stimuli and Apparatus</h2>
<p>From <span class="citation">Behmer and Crump (2017)</span>, “Typists copy-typed five normal paragraphs from the Simple English Wiki, a version of the online encyclopedia Wikipedia written in basic English. Four of the paragraphs were from the entry about cats (<a href="http://simple.wikipedia" class="uri">http://simple.wikipedia</a>. org/wiki/Cat), and one paragraph was from the entry for music (<a href="http://simple.wikipedia.org/wiki/Music" class="uri">http://simple.wikipedia.org/wiki/Music</a>). Each normal paragraph had an average of 131 words (range 124–137).”</p>
<p>The apparatus was a website displaying textbox containing a single paragraph. Paragraph text was black, presented in 14 pt, Helvetica font. JavaScript was used to record keystroke timestamps in milliseconds.</p>
</div>
<div id="design-and-procedure" class="section level2">
<h2>Design and Procedure</h2>
<p>From <span class="citation">Behmer and Crump (2017)</span>, “Participants were instructed to begin typing with the first letter in the paragraph. Correctly typed letters turned green, and typists could only proceed to the next by typing the current letter correctly. After completing the task, participants were presented with a debriefing, and a form to provide any feedback about the task. The task took around 30 to 45 minutes to complete. Participants who completed the task were paid $1.”</p>
</div>
<div id="data-analysis-and-pre-processing" class="section level2">
<h2>Data analysis and pre-processing</h2>
<p>We used R <span class="citation">(Version 3.4.2; R Core Team 2017)</span> and the R-packages <em>bindrcpp</em> <span class="citation">(Version 0.2.2; Müller 2018)</span>, <em>bit</em> <span class="citation">(Version 1.1.14; Oehlschlägel 2018; Oehlschlägel 2017)</span>, <em>bit64</em> <span class="citation">(Version 0.9.7; Oehlschlägel 2017)</span>, <em>bookdown</em> <span class="citation">(Version 0.7; Xie 2016)</span>, <em>Crump</em> <span class="citation">(Version 1.0; Crump 2017)</span>, <em>data.table</em> <span class="citation">(Version 1.11.4; Dowle and Srinivasan 2017)</span>, <em>dplyr</em> <span class="citation">(Version 0.7.4; Wickham et al. 2017)</span>, <em>ggplot2</em> <span class="citation">(Version 3.0.0; Wickham 2009)</span>, <em>ggpubr</em> <span class="citation">(Version 0.1.6; Kassambara 2017)</span>, <em>knitr</em> <span class="citation">(Version 1.20; Xie 2015)</span>, <em>magrittr</em> <span class="citation">(Version 1.5; Bache and Wickham 2014)</span>, <em>papaja</em> <span class="citation">(Version 0.1.0.9655; Aust and Barth 2018)</span>, <em>Rcpp</em> <span class="citation">(Version 0.12.18; Eddelbuettel and François 2011; Eddelbuettel and Balamuta 2017; Eddelbuettel 2017)</span>, <em>RcppZiggurat</em> <span class="citation">(Version 0.1.4; Eddelbuettel 2017)</span>, <em>Rfast</em> <span class="citation">(Version 1.8.8; Papadakis et al. 2018)</span>, <em>rlist</em> <span class="citation">(Version 0.4.6.1; Ren 2016)</span>, and <em>skimr</em> <span class="citation">(Version 1.0.3; McNamara et al. 2018)</span> for all our analyses.</p>
<p>For each subject, we applied the following pre-processing steps. We included IKSIs only for keystrokes involving a lower case letter, and only for correct keystrokes that were preceded by a correct keystroke. Outlier IKSIs were removed for each subject, on a cell-by-cell basis, using the <span class="citation">Van Selst and Jolicoeur (1994)</span> non-recursive moving criterion procedure, which eliminated approximately X% of IKSIs from further analysis.</p>
</div>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<div id="typing-performance" class="section level2">
<h2>Typing Performance</h2>
<pre class="r"><code>library(data.table)
library(dplyr)
library(ggplot2)
library(Crump)  #for standard error function and Van Selst and Jolicouer outlier elimination
library(ggpubr)</code></pre>
<pre class="r"><code>load(&quot;the_data.Rdata&quot;)

# get subject means for each letter position and word length

subject_means &lt;- the_data %&gt;%
  group_by(Subject,word_lengths,let_pos) %&gt;%
  summarize(mean_IKSI = mean(non_recursive_moving(IKSIs)$restricted))

#restrict to 1-9 positions and word lengths
subject_means &lt;- subject_means[subject_means$let_pos &lt; 10, ]
subject_means &lt;- subject_means[subject_means$word_lengths &lt; 10 &amp;
                       subject_means$word_lengths &gt; 0, ]

# make sure numbers are factors
subject_means$Subject &lt;- as.factor(subject_means$Subject)
subject_means$let_pos &lt;- as.factor(subject_means$let_pos)
subject_means$word_lengths &lt;- as.factor(subject_means$word_lengths)
#subject_means&lt;-cbind(subject_means,H=rep(uncertainty_df$H,346))

# design is unbalanced so we create a single factor for a one-way ANOVA
position_length &lt;- as.factor(paste0(subject_means$let_pos,subject_means$word_lengths))
subject_means &lt;- cbind(subject_means, Pos_len =position_length)

# Run the ANOVA

#note very slow with aov and &gt; than 50 subjects
#aov.out&lt;-summary(aov(mean_IKSI ~ Pos_len + Error(Subject/Pos_len), subject_means[1:(45*10),]))

library(Rfast)
iksi_matrix &lt;- matrix(subject_means$mean_IKSI,ncol=45,nrow=346,byrow=T)

rm.anova2&lt;-function (y, logged = FALSE) 
{
  dm &lt;- dim(y)
  d &lt;- dim(y)[2]
  n &lt;- dim(y)[1]
  ina &lt;- rep(1:n, each = d)
  xi &lt;- rep(1:d, n)
  yi &lt;- rowmeans(y)
  yj &lt;- colmeans(y)
  yt &lt;- mean(yi)
  sst &lt;- n * sum((yj - yt)^2)
  yi &lt;- rep(yi, each = d)
  yj &lt;- rep(yj, n)
  ssr &lt;- sum((as.vector(t(y)) - yi - yj + yt)^2)
  dft &lt;- d - 1
  dfs &lt;- n - 1
  dfr &lt;- dft * dfs
  mst &lt;- sst/dft
  msr &lt;- ssr/dfr
  stat &lt;- mst/msr
  pvalue &lt;- pf(stat, dft, dfr, lower.tail = FALSE, log.p = logged)
  list(f=stat, p=pvalue, mse=msr, df1=dft, df2=dfr)
}

Exp1_ANOVA &lt;- rm.anova2(iksi_matrix)</code></pre>
<pre class="r"><code># Get the grand means by averaging over subject means
subject_means &lt;- the_data %&gt;%
  group_by(Subject,word_lengths,let_pos) %&gt;%
  summarize(mean_IKSI = mean(non_recursive_moving(IKSIs)$restricted))

sum_data &lt;- subject_means %&gt;%
  group_by(word_lengths,let_pos) %&gt;%
  summarize(mean_IKSIs = mean(mean_IKSI, na.rm = TRUE),
            SE = stde(mean_IKSI))

# plot the data

sum_data &lt;- sum_data[sum_data$let_pos &lt; 10, ]
sum_data &lt;- sum_data[sum_data$word_lengths &lt; 10 &amp;
                       sum_data$word_lengths &gt; 0, ]

sum_data$let_pos&lt;-as.factor(sum_data$let_pos)
sum_data$word_lengths&lt;-as.factor(sum_data$word_lengths)

limits &lt;- aes(ymax = mean_IKSIs + SE, ymin = mean_IKSIs - SE)

typing_plot1 &lt;- ggplot(sum_data,aes(x=let_pos,
                                    y=mean_IKSIs,
                                    group=word_lengths,
                                    color=word_lengths
                                    ))+
  geom_line()+
  geom_point()+
  geom_errorbar(limits,width=.2)+
  theme_classic()+
  theme(legend.position=&quot;bottom&quot;)+
  xlab(&quot;Letter Position&quot;)+
  ylab(&quot;Mean Interkeystroke Interval (ms)&quot;)+
  scale_colour_grey()+
  labs(color=&#39;Word Length&#39;) +
  ggtitle(&quot;Mean IKSI by Position and Length&quot;)</code></pre>

<pre class="r"><code># compute all t-tests
all_ts_mat &lt;- matrix(0,ncol=45,nrow=45)
all_ps_mat &lt;- matrix(0,ncol=45,nrow=45)
all_mdiffs &lt;- matrix(0,ncol=45,nrow=45)

for( i in 1:45){
  for( j in 1:45){
    temp_t &lt;- t.test(iksi_matrix[,i],iksi_matrix[,j],paired = T, var.equal = T)
    all_ts_mat[i,j] &lt;- temp_t$statistic
    all_ps_mat[i,j] &lt;- temp_t$p.value
    all_mdiffs[i,j] &lt;- temp_t$estimate
  }
}

# 990 total comparisons

bonferonni_alpha &lt;- .05/990
sig_tests &lt;- all_ps_mat&lt; bonferonni_alpha


all_mdiffs[lower.tri(all_mdiffs)] &lt;- NA
diag(all_mdiffs)&lt;-NA
all_mdiffs &lt;- as.data.frame(all_mdiffs)
all_mdiffs$condition &lt;- seq(1,45)
all_mdiffs &lt;- na.omit(melt(all_mdiffs, &#39;condition&#39;, variable_name=&#39;means&#39;))

sig_tests[lower.tri(sig_tests)] &lt;- NA
sig_tests &lt;- as.data.frame(sig_tests)
sig_tests$condition &lt;- seq(1,45)
sig_tests &lt;- na.omit(melt(sig_tests, &#39;condition&#39;, variable_name=&#39;means&#39;))

all_mdiffs&lt;-cbind(all_mdiffs,sig=as.numeric(sig_tests$value))

position&lt;-c(1,1:2,1:3,1:4,1:5,1:6,1:7,1:8,1:9)
word_length&lt;-c(1,rep(2,2),
               rep(3,3),
               rep(4,4),
               rep(5,5),
               rep(6,6),
               rep(7,7),
               rep(8,8),
               rep(9,9))

the_labels &lt;- paste(position,word_length,sep=&quot;|&quot;)
levels(all_mdiffs$variable) &lt;- the_labels
all_mdiffs$condition &lt;- as.factor(all_mdiffs$condition)
levels(all_mdiffs$condition) &lt;- the_labels


typing_plot2 &lt;- ggplot(all_mdiffs, aes(condition, variable)) +
  ggtitle(&#39;Paired comparisons (light grey = significant)&#39;) +
  theme_classic(base_size = 6) +
  xlab(&#39;Condition&#39;) +
  ylab(&#39;Condition&#39;) +
  geom_tile(aes(fill = sig), color=&#39;white&#39;) +
  scale_fill_gradient(low = &#39;darkgrey&#39;, high = &#39;lightgrey&#39;, space = &#39;Lab&#39;) +
  theme(axis.text.x=element_text(angle=90),
        axis.ticks=element_blank(),
        axis.line=element_blank(),
        panel.border=element_blank(),
        panel.grid.major=element_line(color=&#39;#eeeeee&#39;))+ 
  theme(legend.position=&quot;none&quot;)
  #geom_text(aes(label=abs(round(value))),size=1.5)

ggarrange(typing_plot1, typing_plot2, 
          labels = c(&quot;A&quot;, &quot;B&quot;),
          ncol = 2, nrow = 1)</code></pre>
<div class="figure"><span id="fig:typing-mean-iksis-comparisons"></span>
<img src="Entropy_typing_draft_files/figure-html/typing-mean-iksis-comparisons-1.png" alt="Panel A shows mean interkeystroke intervals (ms) for each letter position as a function of word length. Panel B shows paired samples t-tests for all comparisons between means in panel A. Light grey indicates a significant mean difference (Bonferonni corrected)." width="672" />
<p class="caption">
Figure 1: Panel A shows mean interkeystroke intervals (ms) for each letter position as a function of word length. Panel B shows paired samples t-tests for all comparisons between means in panel A. Light grey indicates a significant mean difference (Bonferonni corrected).
</p>
</div>
<p>For each subject, we calculated mean IKSIs as a function of letter position and word length. The letter position and word length factors were not factorially crossed. To determine whether there were differences among the means we submitted the means to a single factor repeated measures design with 45 levels (e.g., letter position|word length: 1|1, 1|2, 2|2, … 9|9). Figure <a href="#fig:typing-mean-iksis-comparisons">1</a> shows mean IKSIs collapsed over subjects, as a function of letter position and word length.</p>
<p>The omnibus test indicated differences among the means were not likely due to chance, F (44,1.51810^{4}) = 276.7423152, MSE = 1269.6566375, p &lt; .001. Visual inspection of Figure <a href="#fig:typing-mean-iksis-comparisons">1</a> shows several trends across the means consistent with first-letter slowing and mid-word slowing reported by Ostry (1983).</p>
<p>Our more important aim was to determine whether variation among these means can be explained by variation in letter uncertainty. For this reason we do not exhaustively discuss all of the possible 990 differences among these 45 conditions. Nevertheless, we conducted all 990 comparisons using bonferroni corrected paired samples t-tests. The results are displayed in Figure <a href="#fig:typing-mean-iksis-comparisons">1</a>B; which shows whether mean differences between conditions were significant, using light gray to indicate comparisons where, p &lt; .05/990 = .0000505. The overwhelming majority of comparisons showed differences unlikely to be produced by chance. This demonstrates systematic effects of letter position and word length on mean interkeystroke interval.</p>
</div>
<div id="letter-uncertainty-by-position-and-word-length" class="section level2">
<h2>Letter Uncertainty by position and word length</h2>
<p>The primary question of interest was whether natural variation in letter uncertainty explains variance in mean IKSI by position and word length. We estimated letter uncertainty by position and word length from Google’s Ngram database <a href="https://books.google.com/ngrams" class="uri">https://books.google.com/ngrams</a>, which provides frequency counts of letters and words occuring in Google’s massive corpus of millions of digitized books. Letter frequency counts for letters a to z, for each position in words from length one to nine, were obtained from Peter Norvig’s website <a href="http://norvig.com/mayzner.html" class="uri">http://norvig.com/mayzner.html</a>.</p>
<p>For each letter frequency distribution, we computed Shannon’s H (entropy) to quantify letter uncertainty. We converted each letter frequency distribution to a probability distribution then calculated H for each distribution. Panel A of Figure <a href="#fig:letter-uncertainty-by-IKSI">2</a> displays estimates of letter uncertainty (H) as a function of letter position and word length. Visual inspection of the graph shows that variation in letter uncertainty maps closely onto variation in mean IKSI (Figure <a href="#fig:typing-mean-iksis-comparisons">1</a>) as a function of position and word length. In particular, letter uncertainty and mean IKSI for position one as a function of word length appear highly similar. And for the remaining positions, letter uncertainty shows an inverted U- shape with greater letter uncertainty in the middle rather than the beginning and endings of words. This suggests that natural variation of letter uncertainty across position and word in English may account for aspects of the first-letter and mid-word slowing phenomena in typing.</p>
</div>
<div id="letter-uncertainty-and-mean-iksi" class="section level2">
<h2>Letter Uncertainty and Mean IKSI</h2>
<p>If the Hick-Hyman law applied to continuous typing we would expect a neat linear relationship between mean IKSIs and letter uncertainty. Panel B of Figure <a href="#fig:letter-uncertainty-by-IKSI">2</a> shows a plot of mean IKSIs taken from all positions and word lengths against letter uncertainty. The scatterplot shows a general trend for mean IKSI to increase as a function of letter uncertainty.</p>
<pre class="r"><code>letter_freqs &lt;- fread(&quot;ngrams1.csv&quot;,integer64=&quot;numeric&quot;)
letter_freqs[letter_freqs==0]&lt;-1

letter_probabilities &lt;- apply(letter_freqs[,2:74],2,function(x){x/sum(x)})

letter_entropies &lt;- apply(letter_probabilities,2,function(x){-1*sum(x*log2(x))})

position&lt;-as.factor(c(1,1:2,1:3,1:4,1:5,1:6,1:7,1:8,1:9))
word_length&lt;-as.factor(c(1,rep(2,2),
               rep(3,3),
               rep(4,4),
               rep(5,5),
               rep(6,6),
               rep(7,7),
               rep(8,8),
               rep(9,9)))

uncertainty_df&lt;-data.frame(H=letter_entropies[11:(11+44)],position,word_length)

#plot

letter_uncertainty_plot1 &lt;- ggplot(uncertainty_df,
                                   aes(x=position,
                                       y=H,
                                       group=word_length,
                                       color=word_length))+
  geom_line()+
  geom_point()+
  theme_classic(base_size = 10)+
  theme(plot.title = element_text(size = rel(1)))+
  theme(legend.position=&quot;bottom&quot;)+
  xlab(&quot;Letter Position&quot;)+
  ylab(&quot;Letter Uncertainty (H)&quot;)+
  labs(color=&#39;Word Length&#39;) +
  scale_colour_grey()+
  ggtitle(&quot;Letter Uncertainty by Position and Length&quot;)</code></pre>

<pre class="r"><code>sum_data&lt;-cbind(sum_data,H=uncertainty_df$H)

letter_uncertainty_plot2 &lt;- ggplot(sum_data,aes(x=H,
                                                y=mean_IKSIs))+
  geom_smooth(method=&quot;lm&quot;, color=&quot;black&quot;, size=.5, alpha=.2)+
  geom_text(aes(label=word_length), nudge_y=-8, size=2.5)+
  geom_point(aes(color=let_pos))+
  #geom_text(aes(x = 2.5, y = 240, label = lm_eqn(lm(mean_IKSIs ~ H, sum_data))), parse = TRUE)+
  theme_classic(base_size = 10)+
  theme(plot.title = element_text(size = rel(1)))+
  theme(legend.position=&quot;bottom&quot;)+
  xlab(&quot;Letter Uncertainty (H)&quot;)+
  ylab(&quot;Mean Interksytroke Interval (ms)&quot;)+
  labs(color=&#39;Letter Position&#39;) +
  scale_colour_grey()+
  ggtitle(&quot;Mean IKSI by Letter Uncertainty&quot;)



ggarrange(letter_uncertainty_plot1, letter_uncertainty_plot2, 
          labels = c(&quot;A&quot;, &quot;B&quot;),
          ncol = 2, nrow = 1)</code></pre>
<div class="figure"><span id="fig:letter-uncertainty-by-IKSI"></span>
<img src="Entropy_typing_draft_files/figure-html/letter-uncertainty-by-IKSI-1.png" alt="Panel A shows measures of letter uncertainty (H, from google n-gram corpus) for each letter position as a function of word length. Panel B plots mean interkeystroke interval (ms) for each letter position and word length as a function of letter uncertainty (H). The word length condition for each dot is represented by the number on the figure, the shade of grey indicates letter position." width="660" />
<p class="caption">
Figure 2: Panel A shows measures of letter uncertainty (H, from google n-gram corpus) for each letter position as a function of word length. Panel B plots mean interkeystroke interval (ms) for each letter position and word length as a function of letter uncertainty (H). The word length condition for each dot is represented by the number on the figure, the shade of grey indicates letter position.
</p>
</div>
<pre class="r"><code>lr_results&lt;-summary(lm(mean_IKSIs~H, sum_data))


subject_means &lt;- the_data %&gt;%
  group_by(Subject,word_lengths,let_pos) %&gt;%
  summarize(mean_IKSI = mean(non_recursive_moving(IKSIs)$restricted))

#restrict to 1-9 positions and word lengths
subject_means &lt;- subject_means[subject_means$let_pos &lt; 10, ]
subject_means &lt;- subject_means[subject_means$word_lengths &lt; 10 &amp;
                                 subject_means$word_lengths &gt; 0, ]

subject_means &lt;- cbind(subject_means,H=rep(uncertainty_df$H,length(unique(subject_means$Subject))))

correlation_data &lt;- subject_means %&gt;%
  group_by(Subject) %&gt;%
  summarize(pearson_r = cor(mean_IKSI,H),
            r_squared = cor(mean_IKSI,H)^2,
            p_value = cor.test(mean_IKSI,H)$p.value)

library(skimr)

skim_with(numeric=list(n=length,mean=mean,sd=sd,SE=stde),append=FALSE)
skim_out&lt;-skim_to_list(correlation_data)

#Means
#p = skim_out$numeric$mean[1]
#r = skim_out$numeric$mean[2]
#r^2 = skim_out$numeric$mean[3]

#SE
#p = skim_out$numeric$SE[1]
#r = skim_out$numeric$SE[2]
#r^2 = skim_out$numeric$SE[3]</code></pre>
<p>A linear regression with group mean IKSIs (collapsed over subjects) as the dependent variable, and letter uncertainty as the independent variable showed a significant positive trend, F(1, 43) = 11.8196302, p = 0.0013, <span class="math inline">\(R^2 =\)</span> 0.2156094 ( meanIKSI = 59.7479213 <span class="math inline">\(+\)</span> 30.492683 <span class="math inline">\(* H\)</span> ). We also conducted separate linear regressions for each subject and found similar results. For example, the mean correlation was r = 0.44 (SE = 0.0085); mean <span class="math inline">\(R^2\)</span> = 0.22 (SE = 0.0072); and mean p = 0.047 (SE = 0.0072).</p>
</div>
<div id="interim-discussion" class="section level2">
<h2>Interim Discussion</h2>
<pre class="r"><code>categorical_position&lt;-as.character(sum_data$let_pos)
categorical_position[categorical_position==&quot;1&quot;]&lt;-&quot;first&quot;
categorical_position[categorical_position!=&quot;first&quot;]&lt;-&quot;other&quot;
categorical_position&lt;-as.factor(categorical_position)
sum_data&lt;-cbind(sum_data,cp=categorical_position)

lr_results_dual&lt;-summary(lm(mean_IKSIs~cp+H, sum_data))</code></pre>
<p>We can conclude that letter uncertainty as a function of position and length explains a small amount variation in mean IKSIs during continuous typing. The present analysis does not provide strong evidence that a process sensitive to letter uncertainty causes both first-letter and mid-word slowing. For example, all of the first position mean IKSIs are longer than mean IKSIs for other positions at comparables levels of letter uncertainty. And, a linear regression on the group mean IKSIs including letter uncertainty and position (first letter vs. other letter) as independent variables explains much more variance, <span class="math inline">\(R^2\)</span> = 0.8636599, p &lt; .001, than the regression only including letter uncertainty.</p>
<p>This pattern invites a dual-process interpretation. For example, first-letter slowing could be explained by a planning process that increases first position IKSIs as a function of word length. Longer words have more letters, thus plan construction and buffering is assumed to take more time before sequence production begins. At the same time, the finding that letter uncertainty does explain some variance in mean IKSI across position suggests that sequence production is also influenced by a process sensitive to letter uncertainty.</p>
</div>
<div id="letter-uncertainty-by-position-word-length-and-n-1-letter-identity" class="section level2">
<h2>Letter Uncertainty by position, word length, and n-1 letter identity</h2>
<p>Determining whether first-letter and mid-word slowing could emerge from a process sensitive to letter uncertainty depends on how letter uncertainty is calculated. Letter uncertainty can be calculated from any discrete probability distribution of letters. In the previous section we somewhat arbitrarily calculated letter uncertainty separately for each letter position in words of length one to nine. However, the number of alternative schemes is vast. For example, we could further conditionalize our postion by word length probability distributions by the letter identities of letters occuring in any position n-1 to n-x, or n+1 to n+y of a specific position. Furthermore, we could conditionalize letter distributions upon any permissible number of preceding or succeeding n-grams (groups of letters).</p>
<p>Although an exhaustive calculation of letter uncertainty is beyond the scope of this paper, we nevertheless took one further step and calculated letter uncertainty by position and word length, conditionalizing upon n-1 letter identity. Fortunately, Norvig (<a href="http://norvig.com/mayzner.html" class="uri">http://norvig.com/mayzner.html</a>) also provided bigram frequency counts from the Google Ngram corpus as a function of position and word length. We calculated letter uncertainty in the following manner. First position letters have no preceding letter, so H as a function of word length was identical to our prior calculation. For letters in positions two to nine, for all word lengths, we calculated H for every n-1 letter identity, and then took the mean H for each position and length. For example, the second position of a two-letter word has a maximum of 26 letter probability distributions, one for each possible n-1 letter (a to z). We calculated H for all n-1 distributions, then took the mean H as our measure of letter uncertainty for each position and word length. Panel A of Figure <a href="#fig:letter-uncertainty-bigram">3</a> shows mean H conditionalized by n-1 letter identity, as a function of letter position and word length.</p>

<pre class="r"><code>library(dplyr)
library(rlist)
library(ggplot2)
library(bit64)

# GET LETTER POSITION 1 H
# load in the excel file from Norvig:
letter_freqs &lt;- fread(&quot;ngrams1.csv&quot;,integer64=&quot;numeric&quot;)
letter_freqs[letter_freqs==0]&lt;-1

get_prob&lt;- function(df) {apply(df,2,function(x){x/sum(x)})}
get_entropies &lt;- function(df){apply(df,2,function(x){-1*sum(x*log2(x))})}

letter_probabilities&lt;-get_prob(letter_freqs[,2:74])
letter_entropies&lt;-get_entropies(letter_probabilities)


let_pos&lt;-c(1,1:2,1:3,1:4,1:5,1:6,1:7,1:8,1:9)
word_lengths&lt;-c(1,rep(2,2),
                rep(3,3),
                rep(4,4),
                rep(5,5),
                rep(6,6),
                rep(7,7),
                rep(8,8),
                rep(9,9))

uncertainty_df&lt;-data.frame(H=letter_entropies[11:(11+44)],let_pos,word_lengths)
uncertainty_df_pos1&lt;-uncertainty_df %&gt;%
  filter(
    let_pos == 1
  )

# GET LETTER POSITION &gt; 1 H
# read in n-gram tsv and clean up
gram_2 &lt;- read.table(&#39;2-gram.txt&#39;,header=TRUE,sep=&quot;\t&quot;)
colnames(gram_2)&lt;- scan(file=&quot;2-gram.txt&quot;,what=&quot;text&quot;,nlines=1,sep=&quot;\t&quot;)

# fix NA level
levels(gram_2$`2-gram`)&lt;-c(levels(gram_2$`2-gram`),as.character(&quot;NA&quot;))
gram_2[is.na(gram_2$`2-gram`),]$`2-gram` = as.character(&quot;NA&quot;)


# find and replace missing combos with 0 
allLet&lt;-c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;,&quot;I&quot;,&quot;J&quot;,&quot;K&quot;,&quot;L&quot;,&quot;M&quot;,&quot;N&quot;,&quot;O&quot;,&quot;P&quot;,&quot;Q&quot;,&quot;R&quot;,&quot;S&quot;,&quot;T&quot;,&quot;U&quot;,&quot;V&quot;,&quot;W&quot;,&quot;X&quot;,&quot;Y&quot;,&quot;Z&quot;)
allCombos&lt;-c()
for (i in 1:length(allLet)){
  for(j in 1:length(allLet)){
    allCombos&lt;-c(allCombos,paste(allLet[i],allLet[j],sep=&quot;&quot;))
  }
}

missing&lt;-allCombos[!allCombos%in%gram_2$`2-gram`]
missing&lt;-cbind(missing,matrix(0,nrow = length(missing), ncol = ncol(gram_2)-1))
colnames(missing)&lt;-colnames(gram_2)
gram_2&lt;-rbind(gram_2,missing)

# change 0s to 1s
gram_2[gram_2 == 0] &lt;- 1

#split bigrams into letter 1 &amp; 2
letters &lt;- data.frame(do.call(&#39;rbind&#39;, strsplit(as.character(gram_2$`2-gram`),&#39;&#39;,fixed=TRUE)))
colnames(letters)&lt;-c(&#39;n-1&#39;,&#39;n&#39;)
names(gram_2)[names(gram_2) == &#39;2-gram&#39;] &lt;- &#39;bigram&#39;
gram_2&lt;-cbind(letters,gram_2)

#remove unnecessary columns
gram_2&lt;-gram_2[,-4:-12]
gram_2&lt;-gram_2[,-40:-56]
gram_2[,4:39]&lt;-apply(gram_2[,4:39],2,function(x){as.numeric(x)})

# GET ENTROPIES
get_prob&lt;- function(df) {apply(df,2,function(x){x/sum(x)})}
get_entropies &lt;- function(df){apply(df,2,function(x){-1*sum(x*log2(x))})}

letter_probabilities&lt;-(with(gram_2,
                            by(gram_2[,4:39],gram_2[,&#39;n-1&#39;], get_prob,simplify= TRUE)
))

letter_entropies&lt;-lapply(letter_probabilities,get_entropies)
letter_entropies&lt;-list.rbind(letter_entropies)

# column means
means&lt;-colMeans(letter_entropies)

# create data frame
let_pos&lt;-c(2:2,2:3,2:4,2:5,2:6,2:7,2:8,2:9)
word_lengths&lt;-c(rep(2,1),
                rep(3,2),
                rep(4,3),
                rep(5,4),
                rep(6,5),
                rep(7,6),
                rep(8,7),
                rep(9,8))

uncertainty_df&lt;-data.frame(H=means,let_pos,word_lengths)
uncertainty_df&lt;-rbind(uncertainty_df,uncertainty_df_pos1)
#gram_2_test&lt;-merge.data.frame(gram_2,letter_entropies,by.x=(&#39;n-1&#39;),by.y=(&#39;n-1&#39;))

uncertainty_df$let_pos&lt;-as.factor(uncertainty_df$let_pos)
uncertainty_df$word_lengths&lt;-as.factor(uncertainty_df$word_lengths)

uncertainty_df&lt;-uncertainty_df[order(uncertainty_df$let_pos),]
uncertainty_df&lt;-uncertainty_df[order(uncertainty_df$word_lengths),]

sum_data &lt;- cbind(sum_data,H_bigram=uncertainty_df$H)

# plot

uncertainty_bigram_plot1 &lt;- ggplot(sum_data,aes(x=position,
                                                y=H_bigram,
                                                group=word_length,
                                                color=word_length))+
  geom_line()+
  geom_point()+
  theme_classic(base_size = 10)+
  theme(plot.title = element_text(size = rel(1)))+
  theme(legend.position=&quot;bottom&quot;)+
  xlab(&quot;Letter Position&quot;)+
  ylab(&quot;Bigram Uncertainty (H)&quot;)+
  labs(color=&#39;Word Length&#39;) +
  scale_colour_grey()+
  ggtitle(&quot;Bigram Uncertainty by Position and Length&quot;)

# analysis

lr_results_bigram&lt;-summary(lm(mean_IKSIs~H_bigram, sum_data))

uncertainty_bigram_plot2 &lt;-ggplot(sum_data,aes(x=H_bigram,
                                               y=mean_IKSIs))+
  #geom_point(aes(color=let_pos))+
  geom_smooth(method=&quot;lm&quot;, color=&quot;black&quot;, size=.5, alpha=.2)+
  geom_text(aes(label=word_length), nudge_y=-8, size=2.5)+
  geom_point(aes(color=let_pos))+
  #geom_text(aes(x = 2.5, y = 240, label = lm_eqn(lm(mean_IKSIs ~ H, sum_data))), parse = TRUE)+
  theme_classic(base_size = 10)+
  theme(plot.title = element_text(size = rel(1)))+
  theme(legend.position=&quot;bottom&quot;)+
  xlab(&quot;Bigram Uncertainty (H)&quot;)+
  ylab(&quot;Mean Interkeystroke Interval (ms)&quot;)+
  labs(color=&#39;Letter Position&#39;) +
  scale_colour_grey()+
  ggtitle(&quot;Mean IKSIs by Bigram Uncertainty&quot;)

ggarrange(uncertainty_bigram_plot1, uncertainty_bigram_plot2, 
          labels = c(&quot;A&quot;, &quot;B&quot;),
          ncol = 2, nrow = 1)</code></pre>
<div class="figure"><span id="fig:letter-uncertainty-bigram"></span>
<img src="Entropy_typing_draft_files/figure-html/letter-uncertainty-bigram-1.png" alt="Panel A shows measures of bigram uncertainty (H, from google n-gram corpus) for each letter position as a function of word length. Panel B plots mean interkeystroke interval (ms) for each letter position and word length as a function of bigram uncertainty (H). The word length condition for each dot is represented by the number on the figure, the shade of grey indicates letter position." width="660" />
<p class="caption">
Figure 3: Panel A shows measures of bigram uncertainty (H, from google n-gram corpus) for each letter position as a function of word length. Panel B plots mean interkeystroke interval (ms) for each letter position and word length as a function of bigram uncertainty (H). The word length condition for each dot is represented by the number on the figure, the shade of grey indicates letter position.
</p>
</div>
<p>Unsurprisingly, letter identity becomes more predictable when n-1 letter identity is known. Compared to the letter uncertainty measures in Figure <a href="#fig:letter-uncertainty-by-IKSI">2</a>, we see that H for letters in positions two to nine is much lower when n-1 letter identity is taken into account. More important, the pattern of H in Figure <a href="#fig:letter-uncertainty-bigram">3</a> much more closely resembles the pattern of mean IKSIs in Figure <a href="#fig:letter-uncertainty-by-IKSI">2</a>.</p>
<p>Panel B of Figure <a href="#fig:letter-uncertainty-bigram">3</a> displays a scatterplot of mean IKSIs as a function of letter uncertainty conditionalized by letter n-1 identity across positions and word length. A linear regression on mean IKSIs using our new measure of letter uncertainty as the independent variable showed a strong positive relationship, F(1, 43) = 182.4438236, p = 4.5e-17, <span class="math inline">\(R^2 =\)</span> 0.8092651 ( meanIKSI = 73.7237131 <span class="math inline">\(+\)</span> 39.3141951 <span class="math inline">\(* H\)</span> ).</p>
</div>
</div>
<div id="an-instance-based-model" class="section level1">
<h1>An instance-based model</h1>
<p>We have shown that variation in mean IKSIs as a function of letter position and word length can be well explained by natural variation in letter uncertainty conditionalized by letter n-1 identity by letter position and word length. This finding licenses consideration of the claim that first-letter and mid-word slowing are caused by a single process sensitive to letter uncertainty. However, the plausibility of this causal claim is empty in the absence of a working process model. Next, we establish theoretical plausibility by showing that letter uncertainty influences on performance can be explained in terms of Logan’s (1988) instance-based memory model of automatization.</p>
<p>Instance theory provides an account of how performance becomes automatized with practice. We will show instance theory’s memory process also develops sensitivity to uncertainty in the stimuli it encounters over practice. More specifically, instance theory’s predictions for performance are nearly identical to the hick-hyman law which posits that reaction times are a linear function of the uncertainty in a choice set.</p>
<p>Instance theory models learning as a function of practice in terms of cue-driven retrieval of stored memory traces <span class="citation">(like other global memory models, Eich 1982; Hintzman 1988; Humphreys et al. 1989)</span>. A new unique trace is preserved in memory every time a response is given to a stimulus. When a familiar stimulus is encountered again, it automatically triggers the retrieval of all stored instances of the stimulus. The timing of the memory-based response to a current stimulus is treated as a race. Whichever memory trace is retrieved first wins the race. As a result, the memory-based reaction time to respond to a stimulus is determined by the retrieval time associated with the fastest memory trace for that stimulus. The retrieval times for every memory trace are assumed to vary, and can be sampled from any desired distribution.</p>
<p>Instance theory models practice based performance speed-ups in terms of sampling extreme values from a growing retrieval time distribution. As the number of memory traces grows the range of the retrieval time distribution also grows, such that he minimum value of the distribution (fastest retrieval time) is more likely to be smaller for distributions with more than fewer memory traces. As a result, reaction times will tend to be faster for more practiced than less practiced stimuli, because more practiced stimuli have a better chance of retrieving a fast memory response than less practiced stimuli.</p>
<p>We can now draw a more transparent connection between instance theory and information theory: they both summarize the same frequency distributions. Information theory provides H as a summary statistic of any discrete probability distributions. Empirical probability distributions for natural occuring stimuli, such as letters, are found by counting stimulus frequencies, and then dividing a stimulus frequency distribution by it’s sum. Instance theory encodes the frequency distributions, and summarizes them at retrieval. It predicts monotonically decreasing RTs as a function of stimulus frequency. Instance theory predictions for a set of stimuli should concord with H by taking the grand mean of the predicted RTs for all choices in a set. We demonstrate these relationships by monte-carlo simulation.</p>
<p>We modeled instance theory predictions for keystroke production times as a function of letter position and word length, for typing natural english text. We treated all 26 letters that could possibly occur in any position for any word length as completely unique and independent stimuli. We modeled the structure of natural english using the 45 letter probability distributions derived from Norvig’s letter frequence counts by position and word length, from Google’s Ngram corpus.</p>
<p>Keystroke times for specific letters were simulated as a function of trace frequency by monte-carlo simulation. We assumed that the retrieval time distribution for each stimulus was sampled from a normal distribution with mean = 500, and standard deviation = 100. Using R, we sampled retrieval times from the normal distribution n times, where n was the current number of memory traces for a given letter. Then we took the minimum value from the sampling distribution as the reaction time. We repeated this process 1000 times to estimate the expected mean reaction time (expected minimum retrieval time) for the given frequency value. In this way, we estimated mean keystroke production times for every letter position across different word lengths. Last, we evaluated model predictions across four practice intervals.</p>
<p>Figure X displays the instance model predictions, across increasing amounts of practice, for mean keystroke production times as a function of letter position and word length. As expected, simulated keystroke times shorten with practice. More imporant, at each stage in practice, simulated keystroke times show the same qualitative pattern of variation across letter postion and word length. Notably, these appear very similar to human typing performance, and to letter uncertainty as a function of position and word length. We conducted linear regressions on simulated mean typing times using letter uncertainty as the independent variable. We found that letter uncertainty nearly perfectly explains the variance in simulated keystroke time, with <span class="math inline">\(R^2\)</span> tending toward 1 with practice.</p>
<p>We point out that the pattern of instance theory predictions for mean reaction time for any set of choice stimuli will be identical to the uncertainty, measured by H, for those sets of choices. This means that instnce theory is a process model of the hick-hyman law, and it inherits both the successess and failures of information theoretic interpretations of performance.</p>
</div>
<div id="general-discussion" class="section level1">
<h1>General Discussion</h1>
<!-- highlights overview -->
<p>Using data from a large N study of continuous typing performance, we reproduced Ostry’s (1983) demonstration that mean interkeystroke interval systematically varies as a function of letter position and word length. We proposed that variation in mean IKSI could be caused by general learning process sensitive to letter uncertainty across position and word length. We calculated letter uncertainty in natural english from Google’s large corpus of digitized text, and showed that it can explain a large portion of the variance ($R^2 = $ 0.8636599) in mean IKSI. Finally, we show that instance theory (Logan, 1988) succesfully models how a general learning and memory process could produce typing performance that is constrained by letter uncertainty. The take home message is that specialized planning processes for motor sequencing in typing are sufficient, but not necessary to explain variation in mean IKSI as a function of letter position and word length; and, that a general learning process provides a more parsimonious account of the data.</p>
<!-- Present inferential limitations -->
<div id="inferential-limitations" class="section level2">
<h2>Inferential limitations</h2>
<p>We have developed a falsifiable causal theory of variation in keystroke dynamics across position and word length for continuous typing of natural english text. However, we are presently limited in the strength of our causal conclusions. Theoretically, we can conclude, by deduction, that an instance model predicts IKSIs will vary as a function of H. Empirically, we have shown correlational evidence that H explains a large portion of the variance in IKSIs across position and word length. However, in this study we did not directly manipulate letter uncertainty by position and word length. Instead, we view the present study as a natural quasi-experimental design, where typists are presumed to be exposed to natural varying conditions of letter uncertainty across position and word length over the course of their experience with typing.</p>
<p>The theory could be tested further in a few ways. For example, if letter uncertainty as a function of letter position and word length varies in different ways across languages, then IKSIs by position and length should also vary across natural languages, following the language-specific pattern of H. Experiments with non-word strings that manipulate the pattern of letter uncertainty across position and word length could also be conducted. Here, IKSIs by position and length should always correspond to the pattern of H. However, it is unclear whether expert typists already familiar with typing in one language would rapidly adapt their performance to the novel letter uncertainty contraints (but see, <span class="citation">Crump and Logan (2010)</span>, for a demonstration that recent episodic experience influences IKSI).</p>
<!-- Implications -->
</div>
<div id="instance-theory-and-skilled-sequential-action" class="section level2">
<h2>Instance theory and skilled sequential action</h2>
<p>Our findings fit well with prior work showing instance-based influences over typing performance, and sequencing in general. For example, borrowing from <span class="citation">(Masson 1986)</span>, <span class="citation">Crump and Logan (2010)</span> showed that recent episodic experience with typing subsets of letters shortens IKSIs for practiced letters, and suggested that letter typing is driven by instance-based retrieval process. <span class="citation">Behmer and Crump (2017)</span> showed that instance theory uniquely predicts, compared to a serial recurrent network model <span class="citation">(Cleeremans 1993; Elman 1990)</span>, changes in sensitivity to letter, bigram, and trigram frequency as a function of typing expertise. <span class="citation">Logan (In Press)</span> has extended instance-based model to account for context-driven automatization over response scheduling typing. Finally, outside of the domain of typing, <span class="citation">Jamieson and Mewhort (2009)</span> showed that an instance-based account explains performance in the serial reaction time task <span class="citation">(Nissen and Bullemer 1987)</span>, a well controlled laboratory sequencing task.</p>
<p>The limitations of the instance-based approach higlight open questions. For example, the core assumption that trace-based retrieval times determine memory-based performance implies that practice, in and of itself, is not necessary for automatization. Practice is one route to automatization because it increases the population of traces, thereby increasing the likelihood the population contains fast retrieval times for traces that could support automatization. Indeed, instance theory allows for single-trial automatization, which occurs when the first trace sampled into memory happens to have a very fast retrieval time (for a demonstration see, <span class="citation">Logan and Klapp (1991)</span>). However, instance theory does not lay bare the factors determining how trace encoding processes could reliably record fast traces to optimize the automatization process.</p>
<p>Nevertheless, instance theory could provide theoretically optimized practice schedules for learning to type in an optimal manner. For example, an instance model could be trained to type any set of texts, and learning curves plotting mean IKSI as a function of text and practice would show how typing skill depends on letter uncertainty in the trained text. The applied question for everyday typists is to determine which training texts (e.g., natural texts, random letter texts, parametrically scaled approximations to natural text) provide optimal transfer of automatized typing performance to natural texts.</p>
<p>Although instance theory allows for single-trial automatization, we suspect a magic encoding wand will not replace extended practice for automatizing typing performance. Instead, instance theory also higlights the critical role of context for automatization. As previously, mentioned typing performance at the keystroke level is highly dependent on the immediate letter context <span class="citation">(Gentner 1982; Salthouse 1986; Shaffer 1973)</span>. Instance theory explains context dependency by assuming traces are stored in a context-dependent fashion. A limitation of instance theory is that it is agnostic, and possibly gratuitous, in specifying which cues in enivronment are used as contexts to conditionalize trace storage and retrieval. Our findings suggest at a minimum, that typists are sensitive to letter uncertainty in a deeply context-specific manner, including context specified by letter position, word length, and letter n-1 identity. The implication is that experience with typing letters in all functional contexts is required for automatizing letter typing, perhaps necessiting extended practice as means to experience all of the contexts.</p>
<p>An important remaining question is to characterize the functional envelope of contextual cues mediating retrieval of stored instances in typing. For example, letter identities or ngram units at positions n-x to n+y may also be effective contextual cues mediating retrieval keystroke retrieval times. We suggest that information theory may be used to characterize the natural horizon of structure surrounding individual letters. For example, when we took letter n-1 identity into account, measures of H across letter position and word length were dramatically reduced from 4.7, because n-1 letter identity is highly predictive of letter n identity. However, we expect that expanding the calculation of letter uncertainty to include more preceding and succeeding letter identities will show the natural envelope of H. In other words, letter identities at some remote location will eventually be unpredictive of letter identities at the current position. We think it would be telling if the natural span of the letter uncertainty envelope maps onto the known rate limiting eye-hand copying spans in typing <span class="citation">(Logan 1983)</span>. For example, typing speed slows down as preview of upcoming letters is restricted <span class="citation">(Hershman and Hillix 1965)</span>, and it remains unknown how the size of the preview window corresponds to the natural span of letter uncertainty conditionalized on succeeding letters. Some rate-limiting aspects of limited preview may not reflect internal processing limitations <span class="citation">(McLeod and Hume 1994; Pashler 1994a; Pashler 1994b)</span>, but external constraints on the value of the information in the preview window.</p>
<!-- Broader Implications-->
</div>
</div>
<div id="broader-implications-and-conlusions" class="section level1">
<h1>Broader implications and Conlusions</h1>
<p>We are optimistic that the tools and approach used here could be succesfully applied to other domains beyond skilled typing. We found that information theory, despite it’s flexibility, was a useful measure of structure in the typing environment. Generalist models of cognitive processes assume that cogniton arises through interaction with a structured environment. In addition to specifying the learning and memory rules that extract the structure, it is equally valuable to improve measurement of structure in the environment. Information theory provides one flexible measurement framework for describing the amount of correlated or redundant structure within any set of units in the environment. When applied judiciously, it becomes theoretically possible to define the limits of what a general learning process could learn from an environment. These limits could be useful for testing generalist theories against special process theories, especially when it can be shown that a cognitive process has more knowledge than the structure in an environment could provide.</p>
<p>Information theory spurred the cognitive revolution <span class="citation">(Hick 1952; Hyman 1953; Miller 1956)</span>, and although it was roundly criticized <span class="citation">(see <span class="citeproc-not-found" data-reference-id="procter_hicks_2018"><strong>???</strong></span>)</span>, we think it has descriptive value useful for characterizing the structure in big data turning the next revolution <span class="citation">(Griffiths 2015)</span>. Our demonstration of correspondence between instance theory <span class="citation">(Logan 1988)</span> and measures of uncertainty adds to the process models capable of accounting for uncertainty mediated phenomena <span class="citation">(for a review see, <span class="citeproc-not-found" data-reference-id="procter_hicks_2018"><strong>???</strong></span>)</span>, and offers principled and falsifiable predictions for future work.</p>

</div>
<div id="references" class="section level1">
<h1>References</h1>
<pre class="r"><code>r_refs(file = &quot;r-references.bib&quot;)</code></pre>
<p> </p>
<div id="refs">
<div id="ref-R-papaja">
<p>Aust, Frederik, and Marius Barth. 2018. <em>papaja: Create APA Manuscripts with R Markdown</em>. <a href="https://github.com/crsh/papaja" class="uri">https://github.com/crsh/papaja</a>.</p>
</div>
<div id="ref-R-magrittr">
<p>Bache, Stefan Milton, and Hadley Wickham. 2014. <em>Magrittr: A Forward-Pipe Operator for R</em>. <a href="https://CRAN.R-project.org/package=magrittr" class="uri">https://CRAN.R-project.org/package=magrittr</a>.</p>
</div>
<div id="ref-behmer_crunching_2017">
<p>Behmer, Lawrence P., and M. J. C. Crump. 2017. “Crunching Big Data with Finger Tips: How Typists Tune Their Performance Towards the Statistics of Natural Language.” In <em>Big Data in Cognitive Science</em>, edited by Michael N. Jones, 319–41.</p>
</div>
<div id="ref-clark_supersizing_2008">
<p>Clark, Andy. 2008. <em>Supersizing the Mind: Embodiment, Action, and Cognitive Extension</em>. OUP USA. <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=n5wRDAAAQBAJ&amp;oi=fnd&amp;pg=PR9&amp;dq=andy+clark&amp;ots=_Cpl448Qox&amp;sig=SC8xkxJaEf7G8EZaGPS0zSDEQX8" class="uri">https://books.google.com/books?hl=en&amp;lr=&amp;id=n5wRDAAAQBAJ&amp;oi=fnd&amp;pg=PR9&amp;dq=andy+clark&amp;ots=_Cpl448Qox&amp;sig=SC8xkxJaEf7G8EZaGPS0zSDEQX8</a>.</p>
</div>
<div id="ref-cleeremans_mechanisms_1993">
<p>Cleeremans, Axel. 1993. <em>Mechanisms of Implicit Learning Connectionist Models of Sequence Processing</em>. Cambridge, Mass.: MIT Press.</p>
</div>
<div id="ref-crump_episodic_2010">
<p>Crump, M. J. C., and Gordon D. Logan. 2010. “Episodic Contributions to Sequential Control: Learning from a Typist’s Touch.” <em>Journal of Experimental Psychology: Human Perception and Performance</em> 36 (3): 662–72. doi:<a href="https://doi.org/10.1037/a0018390">10.1037/a0018390</a>.</p>
</div>
<div id="ref-R-Crump">
<p>Crump, Matthew. 2017. <em>Crump: Crump Lab Functions</em>.</p>
</div>
<div id="ref-R-data.table">
<p>Dowle, Matt, and Arun Srinivasan. 2017. <em>Data.table: Extension of ‘Data.frame‘</em>. <a href="https://CRAN.R-project.org/package=data.table" class="uri">https://CRAN.R-project.org/package=data.table</a>.</p>
</div>
<div id="ref-R-RcppZiggurat">
<p>Eddelbuettel, Dirk. 2017. <em>RcppZiggurat: ’Rcpp’ Integration of Different “Ziggurat” Normal Rng Implementations</em>. <a href="https://CRAN.R-project.org/package=RcppZiggurat" class="uri">https://CRAN.R-project.org/package=RcppZiggurat</a>.</p>
</div>
<div id="ref-R-Rcpp_b">
<p>Eddelbuettel, Dirk, and James Joseph Balamuta. 2017. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” <em>PeerJ Preprints</em> 5 (August): e3188v1. doi:<a href="https://doi.org/10.7287/peerj.preprints.3188v1">10.7287/peerj.preprints.3188v1</a>.</p>
</div>
<div id="ref-R-Rcpp_a">
<p>Eddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” <em>Journal of Statistical Software</em> 40 (8): 1–18. doi:<a href="https://doi.org/10.18637/jss.v040.i08">10.18637/jss.v040.i08</a>.</p>
</div>
<div id="ref-eich_composite_1982">
<p>Eich, Janet M. 1982. “A Composite Holographic Associative Recall Model.” <em>Psychological Review</em> 89 (6): 627–61.</p>
</div>
<div id="ref-elman_finding_1990">
<p>Elman, Jeffrey L. 1990. “Finding Structure in Time.” <em>Cognitive Science</em> 14 (2): 179–211.</p>
</div>
<div id="ref-fodor_modularity_1983">
<p>Fodor, Jerry A. 1983. <em>The Modularity of Mind: An Essay on Faculty Psychology</em>. Cambridge, Mass.: MIT Press.</p>
</div>
<div id="ref-GentnerEvidencecentralcontrol1982">
<p>Gentner, Donald R. 1982. “Evidence Against a Central Control Model of Timing in Typing.” <em>Journal of Experimental Psychology: Human Perception and Performance</em> 8 (6): 793–810. doi:<a href="https://doi.org/10.1037/0096-1523.8.6.793">10.1037/0096-1523.8.6.793</a>.</p>
</div>
<div id="ref-griffiths_manifesto_2015">
<p>Griffiths, Thomas L. 2015. “Manifesto for a New (Computational) Cognitive Revolution.” <em>Cognition</em> 135: 21–23.</p>
</div>
<div id="ref-grudin_digraph_1982">
<p>Grudin, Jonathan T., and Serge Larochelle. 1982. “Digraph Frequency Effects in Skilled Typing.” DTIC Document. <a href="http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA112926" class="uri">http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA112926</a>.</p>
</div>
<div id="ref-heath_stochastic_1990">
<p>Heath, Richard A., and Christopher H. Willcox. 1990. “A Stochastic Model for Inter-Keypress Times in a Typing Task.” <em>Acta Psychologica</em> 75 (1): 13–39.</p>
</div>
<div id="ref-HershmanDataProcessingTyping1965">
<p>Hershman, R. L., and W. A. Hillix. 1965. “Data Processing in Typing: Typing Rate as a Function of Kind of Material and Amount Exposed1.” <em>Human Factors: The Journal of the Human Factors and Ergonomics Society</em> 7 (5): 483–92.</p>
</div>
<div id="ref-hick_rate_1952">
<p>Hick, William E. 1952. “On the Rate of Gain of Information.” <em>Quarterly Journal of Experimental Psychology</em> 4 (1): 11–26. <a href="http://www.tandfonline.com/doi/abs/10.1080/17470215208416600" class="uri">http://www.tandfonline.com/doi/abs/10.1080/17470215208416600</a>.</p>
</div>
<div id="ref-hintzman_judgments_1988">
<p>Hintzman, Douglas L. 1988. “Judgments of Frequency and Recognition Memory in a Multiple-Trace Memory Model.” <em>Psychological Review</em> 95 (4): 528.</p>
</div>
<div id="ref-humphreys_global_1989">
<p>Humphreys, Michael S., Ray Pike, John D. Bain, and Gerald Tehan. 1989. “Global Matching: A Comparison of the SAM, Minerva II, Matrix, and TODAM Models.” <em>Journal of Mathematical Psychology</em> 33 (1): 36–67.</p>
</div>
<div id="ref-hyman_stimulus_1953">
<p>Hyman, Ray. 1953. “Stimulus Information as a Determinant of Reaction Time.” <em>Journal of Experimental Psychology</em> 45 (3): 188. <a href="http://psycnet.apa.org/journals/xge/45/3/188/" class="uri">http://psycnet.apa.org/journals/xge/45/3/188/</a>.</p>
</div>
<div id="ref-JacobyNonanalyticcognitionMemory1984">
<p>Jacoby, Larry L., and Lee R. Brooks. 1984. “Nonanalytic Cognition: Memory, Perception, and Concept Learning.” <em>The Psychology of Learning and Motivation</em> 18: 1–47.</p>
</div>
<div id="ref-jamieson_applying_2009">
<p>Jamieson, Randall K., and D. J. K. Mewhort. 2009. “Applying an Exemplar Model to the Serial Reaction-Time Task: Anticipating from Experience.” <em>The Quarterly Journal of Experimental Psychology</em> 62 (9): 1757–83. <a href="http://www.tandfonline.com/doi/abs/10.1080/17470210802557637" class="uri">http://www.tandfonline.com/doi/abs/10.1080/17470210802557637</a>.</p>
</div>
<div id="ref-john_typist:_1996">
<p>John, Bonnie E. 1996. “TYPIST: A Theory of Performance in Skilled Typing.” <em>Human-Computer Interaction</em> 11 (4): 321–55.</p>
</div>
<div id="ref-R-ggpubr">
<p>Kassambara, Alboukadel. 2017. <em>Ggpubr: ’Ggplot2’ Based Publication Ready Plots</em>. <a href="https://CRAN.R-project.org/package=ggpubr" class="uri">https://CRAN.R-project.org/package=ggpubr</a>.</p>
</div>
<div id="ref-KolersProceduresmind1984">
<p>Kolers, Paul A., and Henry L. Roediger. 1984. “Procedures of Mind.” <em>Journal of Verbal Learning and Verbal Behavior</em> 23 (4): 425–49.</p>
</div>
<div id="ref-logan_span_1983">
<p>Logan, Gordan D. 1983. “Time, Information, and the Various Spans in Typewriting.” In <em>Cognitive Aspects of Skilled Typewriting</em>, edited by W. E. Cooper, 197–224.</p>
</div>
<div id="ref-logan_toward_1988">
<p>Logan, Gordon D. 1988. “Toward an Instance Theory of Automatization.” <em>Psychological Review</em> 95 (4): 492–527. <a href="http://psycnet.apa.org/journals/rev/95/4/492/" class="uri">http://psycnet.apa.org/journals/rev/95/4/492/</a>.</p>
</div>
<div id="ref-logan_inpress">
<p>———. In Press. “Automatic Control: How Experts Act Without Thinking.” <em>Psychological Review</em> TBD (TBD): TBD.</p>
</div>
<div id="ref-logan_hierarchical_2011">
<p>Logan, Gordon D., and M. J. C. Crump. 2011. “Hierarchical Control of Cognitive Processes: The Case for Skilled Typewriting.” In <em>Psychology of Learning and Motivation</em>, edited by B. H. Ross, 54:1–27. Elsevier. <a href="https://CrumpLab.github.io/CognitionPerformanceLab/CrumpPubs/Logan and Crump - 2011.pdf" class="uri">https://CrumpLab.github.io/CognitionPerformanceLab/CrumpPubs/Logan and Crump - 2011.pdf</a>.</p>
</div>
<div id="ref-logan_automatizing_1991">
<p>Logan, Gordon D., and Stuart T. Klapp. 1991. “Automatizing Alphabet Arithmetic: I. Is Extended Practice Necessary to Produce Automaticity?” <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em> 17 (2): 179–95.</p>
</div>
<div id="ref-massaro_typing_1984">
<p>Massaro, Dominic W., and Peter A. Lucas. 1984. “Typing Letter Strings Varying in Orthographic Structure.” <em>Acta Psychologica</em> 57 (2): 109–31.</p>
</div>
<div id="ref-MassonIdentificationtypographicallytransformed1986">
<p>Masson, Michael E. 1986. “Identification of Typographically Transformed Words: Instance-Based Skill Acquisition.” <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em> 12 (4): 479.</p>
</div>
<div id="ref-mcleod_overlapping_1994">
<p>McLeod, Peter, and Mark Hume. 1994. “Overlapping Mental Operations in Serial Performance with Preview: Typing. A Reply to Pashler.” <em>The Quarterly Journal of Experimental Psychology</em> 47 (1): 193–99.</p>
</div>
<div id="ref-R-skimr">
<p>McNamara, Amelia, Eduardo Arino de la Rubia, Hao Zhu, Shannon Ellis, and Michael Quinn. 2018. <em>Skimr: Compact and Flexible Summaries of Data</em>. <a href="https://CRAN.R-project.org/package=skimr" class="uri">https://CRAN.R-project.org/package=skimr</a>.</p>
</div>
<div id="ref-miller_magical_1956">
<p>Miller, George A. 1956. “The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information.” <em>Psychological Review</em> 63 (2): 81–97. <a href="http://psycnet.apa.org/journals/rev/63/2/81/" class="uri">http://psycnet.apa.org/journals/rev/63/2/81/</a>.</p>
</div>
<div id="ref-R-bindrcpp">
<p>Müller, Kirill. 2018. <em>Bindrcpp: An ’Rcpp’ Interface to Active Bindings</em>. <a href="https://CRAN.R-project.org/package=bindrcpp" class="uri">https://CRAN.R-project.org/package=bindrcpp</a>.</p>
</div>
<div id="ref-NissenAttentionalrequirementslearning1987">
<p>Nissen, Mary Jo, and Peter Bullemer. 1987. “Attentional Requirements of Learning: Evidence from Performance Measures.” <em>Cognitive Psychology</em> 19 (1): 1–32.</p>
</div>
<div id="ref-R-bit64">
<p>Oehlschlägel, Jens. 2017. <em>Bit64: A S3 Class for Vectors of 64bit Integers</em>. <a href="https://CRAN.R-project.org/package=bit64" class="uri">https://CRAN.R-project.org/package=bit64</a>.</p>
</div>
<div id="ref-R-bit">
<p>———. 2018. <em>Bit: A Class for Vectors of 1-Bit Booleans</em>. <a href="https://CRAN.R-project.org/package=bit" class="uri">https://CRAN.R-project.org/package=bit</a>.</p>
</div>
<div id="ref-OstryDeterminantsinterkeytimes1983">
<p>Ostry, David J. 1983. “Determinants of Interkey Times in Typing.” <em>Cognitive Aspects of Skilled Typewriting</em>, 225–46.</p>
</div>
<div id="ref-R-Rfast">
<p>Papadakis, Manos, Michail Tsagris, Marios Dimitriadis, Stefanos Fafalios, Ioannis Tsamardinos, Matteo Fasiolo, Giorgos Borboudakis, John Burkardt, Changliang Zou, and Kleanthi Lakiotaki. 2018. <em>Rfast: A Collection of Efficient and Extremely Fast R Functions</em>. <a href="https://CRAN.R-project.org/package=Rfast" class="uri">https://CRAN.R-project.org/package=Rfast</a>.</p>
</div>
<div id="ref-pashler_comment_1994">
<p>Pashler, Harold. 1994a. “Comment on McLeod and Hume, Overlapping Mental Operations in Serial Performance with Preview: Typing.” <em>The Quarterly Journal of Experimental Psychology</em> 47 (1): 201–5.</p>
</div>
<div id="ref-pashler_overlapping_1994">
<p>———. 1994b. “Overlapping Mental Operations in Serial Performance with Preview.” <em>The Quarterly Journal of Experimental Psychology</em> 47 (1): 161–91.</p>
</div>
<div id="ref-PinetTypingwritingLinguistic2016">
<p>Pinet, Svetlana, Johannes C. Ziegler, and F.-Xavier Alario. 2016. “Typing Is Writing: Linguistic Properties Modulate Typing Execution.” <em>Psychonomic Bulletin &amp; Review</em> 23 (6): 1898–1906.</p>
</div>
<div id="ref-proctor_hicks_2018">
<p>Proctor, Robert W, and Darryl W Schneider. 2018. “Hick’s Law for Choice Reaction Time: A Review.” <em>Quarterly Journal of Experimental Psychology</em> 71 (6): 1281–99. doi:<a href="https://doi.org/10.1080/17470218.2017.1322622">10.1080/17470218.2017.1322622</a>.</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2017. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-R-rlist">
<p>Ren, Kun. 2016. <em>Rlist: A Toolbox for Non-Tabular Data Manipulation</em>. <a href="https://CRAN.R-project.org/package=rlist" class="uri">https://CRAN.R-project.org/package=rlist</a>.</p>
</div>
<div id="ref-rumelhart_parallel_1986">
<p>Rumelhart, David E., and James L. McClelland. 1986. <em>Parallel Distributed Processing, Explorations in the Microstructure of Cognition, Volume 1: Foundations</em>. Cambridge, Mass. [u.a.]: MIT Press.</p>
</div>
<div id="ref-RumelhartSimulatingskilledtypist1982">
<p>Rumelhart, David E., and Donald A. Norman. 1982. “Simulating a Skilled Typist: A Study of Skilled Cognitive-Motor Performance.” <em>Cognitive Science</em> 6 (1): 1–36.</p>
</div>
<div id="ref-salthouse_effects_1984">
<p>Salthouse, Timothy A. 1984. “Effects of Age and Skill in Typing.” <em>Journal of Experimental Psychology: General</em> 113 (3): 345.</p>
</div>
<div id="ref-salthouse_perceptual_1986">
<p>———. 1986. “Perceptual, Cognitive, and Motoric Aspects of Transcription Typing.” <em>Psychological Bulletin</em> 99 (3): 303–19.</p>
</div>
<div id="ref-shaffer_latency_1973">
<p>Shaffer, L. H. 1973. “Latency Mechanisms in Transcription.” <em>Attention and Performance IV</em>, 435–46.</p>
</div>
<div id="ref-shaffer_typing_1968">
<p>Shaffer, L. H., and J. Hardwick. 1968. “Typing Performance as a Function of Text.” <em>The Quarterly Journal of Experimental Psychology</em> 20 (4): 360–69.</p>
</div>
<div id="ref-Shannonmathematicaltheorycommunication1998">
<p>Shannon, Claude E., and Warren Weaver. 1998. <em>The Mathematical Theory of Communication</em>. University of Illinois press.</p>
</div>
<div id="ref-terzuolo_determinants_1980">
<p>Terzuolo, C. A., and P. Viviani. 1980. “Determinants and Characteristics of Motor Patterns Used for Typing.” <em>Neuroscience</em> 5 (6): 1085–1103.</p>
</div>
<div id="ref-van_selst_solution_1994">
<p>Van Selst, M., and P. Jolicoeur. 1994. “A Solution to the Effect of Sample Size on Outlier Elimination.” <em>The Quarterly Journal of Experimental Psychology</em> 47A: 631–50.</p>
</div>
<div id="ref-vinson_quantifying_2017">
<p>Vinson, David W. 2017. “Quantifying Context and Its Effects in Large Natural Datasets.” PhD Thesis, University of California, Merced.</p>
</div>
<div id="ref-R-ggplot2">
<p>Wickham, Hadley. 2009. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. <a href="http://ggplot2.org" class="uri">http://ggplot2.org</a>.</p>
</div>
<div id="ref-R-dplyr">
<p>Wickham, Hadley, Romain Francois, Lionel Henry, and Kirill Müller. 2017. <em>Dplyr: A Grammar of Data Manipulation</em>. <a href="https://CRAN.R-project.org/package=dplyr" class="uri">https://CRAN.R-project.org/package=dplyr</a>.</p>
</div>
<div id="ref-will_linguistic_2006">
<p>Will, Udo, Guido Nottbusch, and Rüdiger Weingarten. 2006. “Linguistic Units in Word Typing: Effects of Word Presentation Modes and Typing Delay.” <em>Written Language &amp; Literacy</em> 9 (1): 153–76.</p>
</div>
<div id="ref-wu_queuing_2008">
<p>Wu, Changxu, and Yili Liu. 2008. “Queuing Network Modeling of Transcription Typing.” <em>ACM Transactions on Computer-Human Interaction</em> 15 (1): 1–45. doi:<a href="https://doi.org/10.1145/1352782.1352788">10.1145/1352782.1352788</a>.</p>
</div>
<div id="ref-R-knitr">
<p>Xie, Yihui. 2015. <em>Dynamic Documents with R and Knitr</em>. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. <a href="https://yihui.name/knitr/" class="uri">https://yihui.name/knitr/</a>.</p>
</div>
<div id="ref-R-bookdown">
<p>———. 2016. <em>Bookdown: Authoring Books and Technical Documents with R Markdown</em>. Boca Raton, Florida: Chapman; Hall/CRC. <a href="https://github.com/rstudio/bookdown" class="uri">https://github.com/rstudio/bookdown</a>.</p>
</div>
</div>
<p></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
