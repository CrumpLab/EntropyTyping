---
title: "Learning about the statistics of the environment"
subtitle: "Some evidence from typing studies"
author: "Matthew Crump"
date: "2018/07/20 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["defaultb.css", "metropolisb.css", "metropolis-fontsb.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE,message=FALSE,warning=FALSE, cache = TRUE)
```



class: pink, center, middle, clear

# A bit of research, and a bit of process and tools

---


class: pink, center, middle, clear

# Two views on cognition

---

# Where is the complexity?

.pull-left[

## Cognitive Processes

]

.pull-right[

## Patterns in the world

]

---

# My general research interests

1. How basic learning and memory processes become sensitive to complex patterns in the world

--

2. How cognitive abilities emerge from learning about patterns in the world

---

# Question

If you were to take a random book off the shelf (written in English), open it at a random location, and point randomly to a single letter...

Would you be able to predict the identity of the letter?

---

# What letter should you pick?

.center[The **most frequent letter**]

---

# Letter frequency distribution

```{r, out.width="80%"}
knitr::include_graphics("figs/letterfreq.png")
```

---

# Entropy (H)

Claude Shannon (1949) developed a formula to measure the total predictability in a discrete probability distribution

$H = - \sum{p_i log_2 p_i}$

H= 0 when things are perfectly predictable
H= it's maximum value when things are perfectly unpredictable

---

# No predictability

.pull-left[

```{r}
letters<-c("a","b","c","d","e","f","g","h","i","j","k","l",
          "m","n","o","p","q","r","s","t","u","v","w","x","y","z")
probabilities <- rep(1/26,26)
df <- data.frame(letters,probabilities)
H<-(-1)*sum(probabilities*log2(probabilities))
library(ggplot2)
ggplot(df, aes(x=letters, y=probabilities))+
  geom_bar(stat="identity")+
  theme_classic(base_size=20)+
  coord_cartesian(ylim=c(0,.5))

```

]

.pull-right[

## H= `r H`
]

---

# more predictability

.pull-left[

```{r}
letters<-c("a","b","c","d","e","f","g","h","i","j","k","l",
          "m","n","o","p","q","r","s","t","u","v","w","x","y","z")
probabilities <- c(rep(1/26,20),.24,0.01,0.01,0.01,0.01,0.01)
df <- data.frame(letters,probabilities)
H<-(-1)*sum(probabilities*log2(probabilities))
library(ggplot2)
ggplot(df, aes(x=letters, y=probabilities))+
  geom_bar(stat="identity")+
  theme_classic(base_size=20)+
  coord_cartesian(ylim=c(0,.5))

```

]

.pull-right[

## H= `r H`
]

---


# A lot of  predictability

.pull-left[

```{r}
letters<-c("a","b","c","d","e","f","g","h","i","j","k","l",
          "m","n","o","p","q","r","s","t","u","v","w","x","y","z")
probabilities <- c(rep(.01,22),.22,.22,.22,.22)
df <- data.frame(letters,probabilities)
H<-(-1)*sum(probabilities*log2(probabilities))
library(ggplot2)
ggplot(df, aes(x=letters, y=probabilities))+
  geom_bar(stat="identity")+
  theme_classic(base_size=20)+
  coord_cartesian(ylim=c(0,.5))

```

]

.pull-right[

## H= `r H`
]

---

# We can use H

```{r, out.width="80%"}
knitr::include_graphics("figs/letterfreq.png")
```

---

[Online paper](https://crumplab.github.io/EntropyTyping/Entropy_typing_draft.html)
