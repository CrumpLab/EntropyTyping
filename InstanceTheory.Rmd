---
title: "Instance_theory"
author: "Matt Crump"
date: "6/13/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Instance theory of automatization for 1 S-R pair

The idea:

Everytime you practice this one stimulus-response pair (let's say seeing A, and typing A), you store a trace of that experience in memory. One trace for each each experience.

Assumption: Cue-driven retrieval

Every time to see the stimulus A, it causes the retrieval of the instances in memory of you previously responding to that stimulus. In this way, you can let your memory for your previous response guide your response to the current stimulus

Assumption: Each trace has it's own retrieval time

The speed of memory retrieval depends on a winner-takes-all face. When you see A, all of the traces in memory for A get retrieved. BUT, they all get retrieved with different speeds. Some traces come back faster, and some slower. Naturally, the fastest single trace comes back first.

Modelling the practice curve:

At each step in practice, you have one more memory trace in your memory. Each memory trace has a unique retrieval time that is sampled from some distribution. The fastes one always wins. In general, the more instances you have in memory, the more fast traces you will. So, if performance (that is driven by memory retrieval) depends on the speed of the fastest memory, then performance will gradually get faster with practice, because across practice people will have a higher probablility of storing a faster and faster memory trace.

Some code:

```{r}

performance <- c()

for (traces in 1:500){
  memory_retrieval_time <- min(rnorm(traces, mean = 500, sd = 100))
  performance           <- c(performance,memory_retrieval_time)
}

plot(performance)

```

The above get's the idea across, but it's a hacky approach. None of the individual memories are saved, here is a different way:

```{r}

# get 100 memory traces, across 100 practice attempts, each with their own retrieval time

memory_retrieval_times <- rnorm(100, mean = 500, sd = 100) # we could use a differnet distribution if we wanted

performance <- length(100)

for(trial in 1:100){
  performance[trial] <- min(memory_retrieval_times[1:trial]) # fastest one always wins
}

plot(performance)


```

Interesting, that this way of doing it produces step-functions. At any point in practice, whichever trace has the fastest retrieval time always wins and determines speed of performance. You can for stretches in practice where the new memories do not have faster retrieval times than the fastest existing memory. Let's look at this over 1,000 trials.

```{r}

memory_retrieval_times <- rnorm(1000, mean = 500, sd = 100) # we could use a differnet distribution if we wanted

performance <- length(1000)

for(trial in 1:1000){
  performance[trial] <- min(memory_retrieval_times[1:trial]) # fastest one always wins
}

plot(performance)


```

Let's do the same as above, but now imagine we are doing it for many different participants, say 10 different participants.

```{r}

all_performance<-c()

for (subjects in 1:10) {
  
  memory_retrieval_times <- rnorm(1000, mean = 500, sd = 100) # we could use a differnet distribution if we wanted
  
  performance <- length(1000)
  
  for(trial in 1:1000){
    performance[trial] <- min(memory_retrieval_times[1:trial]) # fastest one always wins
  }
  
all_performance<-c(all_performance,performance)
  
}

subject_df <- data.frame(subject=rep(1:10,each=1000),
                         trial = rep(1:1000,10),
                         performance = all_performance)

library(ggplot2)

ggplot(subject_df, aes(x=trial,y=performance))+
  geom_point()+
  theme_classic()+
  facet_wrap(~subject)


```


We can see that different simulated subjects in the model have different learning curves. Some subjects are faster from the beginning of practice, why does this occur? In the model, if a subject happened, by random chance, to have a first trace that had a fast retrieval time, this single trace would control performance for many trials, until a new memory trace with an even faster retrieval time happens to be sampled into memory.

This notion shows an interesting implication of instance theory. You don't need practice, you just need a fast memory trace. If there was some subject who happened to store a memory trace with a really fast retrieval time on the first trial, say 100 ms, then they would not have much of a learning curve at all. They would be responding on at 100ms for the entire duration of practice, because that single memory trace would always win the race to control performance. 

Logan and Klapp, wrote a paper on this and showed some evidence in support of this kind of single-trial learning. Neato.

Logan, G. D., & Klapp, S. T. (1991). Automatizing alphabet arithmetic: I. Is extended practice necessary to produce automaticity?. Journal of Experimental Psychology: Learning, Memory, and Cognition, 17(2), 179.

# Instance theory and Information Theory

I'm not aware of work relating instance theory directly to information theory. Gordon didn't reference Hick, Hyman, or Shannon in his major instance theory papers. Maybe he has talked about the relationship elsewhere, we should ask him...

Jamieson & Mewhort's (2009) model of the SRT task (serial reaction time task) uses a different kind of instance theory (Hintzman's MINERVA), and they talk about the relationship. To grossly summarize, MINERVA is sensitive to the information in the stimuli that are preserved in it's memories.

As I think about this in terms of Logan's model, the relationship seems pretty straightforward. Instance theory should be sensitive to information in the choice set. If we unpack this with some R code, we should see the relationships more clearly.

Instance theory learns as a function of the frequency of responding to specific events. It ends up being a frequency model in the long run, even though you don't need to have a lot of experience (you just need one fast instance). Information theory provides a summary statistic to quantify uncertainty in a set of choices. The uncertainty in the choices is really just a summary statistic of the frequencies with which the choices occur. More formally, H summarizes the number of bits needed to represent the probability distribution for the choice set. We get the probability distribution from the frequencies. With this in mind, we should expect some correspondence between predictions from instance theory about how performacne depends on choice frequencies, and statements from information theory about the amount of uncertainty in the choice sets that people are learning.

## Is an instance-based process sensitive to uncertainty?

It should be. Consider two learning situations, both involving learning to respond to one of four stimuli (a, b, c, d). We can create a high entropy task where all of the choices are random and equally probable H=2, and we can create a lower entropy task where some of the options are more probable than the others. Then we can have instance models practice these tasks and see if the models are sensitive to the amount of uncertainty. For example, do simulated subjects learn faster when uncertainty is low compared to when it is high?

```{r}
library(dplyr)

high_entropy <- c(.25,.25,.25,.25)

save_z<-length(10)
for(j in 1:50){

high_entropy_trials <- sample(c(1,2,3,4),1000,prob=high_entropy,replace=T)

high_entropy_df <- data.frame(trials=c(1:1000),
                             item = high_entropy_trials,
                             retrieval_time = rnorm(1000, mean=500, sd=100))

# add running counts of each item
high_entropy_df <- high_entropy_df %>%
                      group_by(item) %>%
                      mutate(Count=row_number())

#probably a nonloop way to do this, oh well
average_RT <- length(1000)
for(i in 1:1000){
  average_RT[i] <- min(rnorm(high_entropy_df$Count[i], mean = 500, sd = 100))
}

high_entropy_df <- cbind(high_entropy_df, RT = average_RT)

model.out<-summary(nls(RT~b*trials^z,start = list(b = 500, z = 1),data=high_entropy_df))
save_z[j] <- model.out$coefficients[2,1]

}

mean(save_z)


low_entropy <- c(.7,.1,.1,.1)

save_z<-length(10)
for(j in 1:50){

low_entropy_trials <- sample(c(1,2,3,4),1000,prob=low_entropy,replace=T)

low_entropy_df <- data.frame(trials=c(1:1000),
                             item = low_entropy_trials,
                             retrieval_time = rnorm(1000, mean=500, sd=100))

# add running counts of each item
low_entropy_df <- low_entropy_df %>%
                      group_by(item) %>%
                      mutate(Count=row_number())

#probably a nonloop way to do this, oh well
average_RT <- length(1000)
for(i in 1:1000){
  average_RT[i] <- min(rnorm(low_entropy_df$Count[i], mean = 500, sd = 100))
}

low_entropy_df <- cbind(low_entropy_df, RT = average_RT)

model.out<-summary(nls(RT~b*trials^z,start = list(b = 500, z = 1),data=low_entropy_df))
save_z[j] <- model.out$coefficients[2,1]

}

mean(save_z)


low_entropy <- c(.97,.01,.01,.01)

save_z<-length(10)
for(j in 1:50){

low_entropy_trials <- sample(c(1,2,3,4),1000,prob=low_entropy,replace=T)

low_entropy_df <- data.frame(trials=c(1:1000),
                             item = low_entropy_trials,
                             retrieval_time = rnorm(1000, mean=500, sd=100))

# add running counts of each item
low_entropy_df <- low_entropy_df %>%
                      group_by(item) %>%
                      mutate(Count=row_number())

#probably a nonloop way to do this, oh well
average_RT <- length(1000)
for(i in 1:1000){
  average_RT[i] <- min(rnorm(low_entropy_df$Count[i], mean = 500, sd = 100))
}

low_entropy_df <- cbind(low_entropy_df, RT = average_RT)

model.out<-summary(nls(RT~b*trials^z,start = list(b = 500, z = 1),data=low_entropy_df))
save_z[j] <- model.out$coefficients[2,1]

}

mean(save_z)


```

Hmm, this above code seems rather complicated for trying to get the point across. But, so far, when we fit a power function to a high, low, and lower entropy choice set, the exponent z gets more negative, indicating a steeper learning curve. In other words, the simulation shows that instance theory learns faster as entropy decreases.

It would be nice a more clever and concise function, where we could take a probability distribution for say 26 items, and then just compute expected levels of average performance for that whole given some number for total practice.




