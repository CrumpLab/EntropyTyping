<!DOCTYPE html>
<html>
  <head>
    <title>Learning about the statistics of the environment</title>
    <meta charset="utf-8">
    <meta name="author" content="Matthew Crump" />
    <link rel="stylesheet" href="defaultb.css" type="text/css" />
    <link rel="stylesheet" href="metropolisb.css" type="text/css" />
    <link rel="stylesheet" href="metropolis-fontsb.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Learning about the statistics of the environment
## Some evidence from typing studies
### Matthew Crump
### 2018/07/20 (updated: 2018-10-11)

---






class: pink, center, middle, clear

# A bit of research, and a bit of process and tools

---


class: pink, center, middle, clear

# Two views on cognition

---

# Where is the complexity?

.pull-left[

## Cognitive Processes

]

.pull-right[

## Patterns in the world

]

---

# My general research interests

1. How basic learning and memory processes become sensitive to complex patterns in the world

--

2. How cognitive abilities emerge from learning about patterns in the world

---

# Question

If you were to take a random book off the shelf (written in English), open it at a random location, and point randomly to a single letter...

Would you be able to predict the identity of the letter?

---

# What letter should you pick?

.center[The **most frequent letter**]

---

# Letter frequency distribution

&lt;img src="figs/letterfreq.png" width="80%" /&gt;

---

# Entropy (H)

Claude Shannon (1949) developed a formula to measure the total predictability in a discrete probability distribution

`\(H = - \sum{p_i log_2 p_i}\)`

H= 0 when things are perfectly predictable
H= it's maximum value when things are perfectly unpredictable

---

# No predictability

.pull-left[

![](prosem_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

]

.pull-right[

## H= 4.7004397
]

---

# more predictability

.pull-left[

![](prosem_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

]

.pull-right[

## H= 4.4420502
]

---


# A lot of  predictability

.pull-left[

![](prosem_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

]

.pull-right[

## H= 3.383942
]

---

# We can use H

&lt;img src="figs/letterfreq.png" width="80%" /&gt;

---

[Online paper](https://crumplab.github.io/EntropyTyping/Entropy_typing_draft.html)
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
