---
title: "Nicks's Analysis"
author: "Nick Brosowsky"
date: "6/19/2018"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,fig.path='Nick_figures/', dev=c('png', 'pdf'))

library(data.table)
library(dplyr)
library(ggplot2)
library(Crump) #for standard error function and Van Selst and Jolicouer outlier elimination
library(rlist)
library(bit64)

```



```{r}

# mturk.txt is the unzipped mturk.txt.zip file
the_data <- fread("mturk.txt")

################
# Data-Exclusion

the_data[grepl("[[:punct:]]",substr(the_data$whole_word,nchar(the_data$whole_word),nchar(the_data$whole_word))),]$word_lengths=the_data[grepl("[[:punct:]]",substr(the_data$whole_word,nchar(the_data$whole_word),nchar(the_data$whole_word))),]$word_lengths-1

the_data <- the_data %>%
             filter (
                      Letters != " ",                 #removes spaces (just in case they were assigned a letter position)
                      !grepl("[[:punct:]]",Letters),  #removes punctuation
                      !grepl("[0-9]",Letters),        #removes numbers
                      !grepl("[[A-Z]]*",Letters),   #removes Letters that have a capital letter
                      ParagraphType == "N",
                      PredBigramCorrect == "11",
                      IKSIs < 2000
             )


###############
# Analysis
# Get the means by word length and letter position for each subject
# Use Van Selst and Jolicouer non-recursive_moving procedure from Crump

subject_means <- the_data %>%
              group_by(Subject,word_lengths,let_pos) %>%
              summarize(mean_IKSI = mean(non_recursive_moving(IKSIs)$restricted))

# Get the grand means by averaging over subject means
sum_data <- subject_means %>%
              group_by(word_lengths,let_pos) %>%
              summarize(mean_IKSIs = mean(mean_IKSI, na.rm = TRUE),
                        SE = stde(mean_IKSI))

# plot the data

sum_data <- sum_data[sum_data$let_pos < 10, ]
sum_data <- sum_data[sum_data$word_lengths < 10 &
                     sum_data$word_lengths > 0, ]

sum_data$let_pos<-as.factor(sum_data$let_pos)
sum_data$word_lengths<-as.factor(sum_data$word_lengths)

limits <- aes(ymax = mean_IKSIs + SE, ymin = mean_IKSIs - SE)

ggplot(sum_data,aes(x=let_pos,y=mean_IKSIs,group=word_lengths,color=word_lengths))+
  geom_line()+
  geom_point()+
  geom_errorbar(limits,width=.2)+
  theme_classic()+
  ggtitle("Mean IKSI as a Function of Letter Position and Word Length")


```


# NEW letter uncertainty

```{r}


## GET LETTER POSITION 1 H
# load in the excel file from Norvig:
letter_freqs <- fread("ngrams1.csv",integer64="numeric")
letter_freqs[letter_freqs==0]<-1

get_prob<- function(df) {apply(df,2,function(x){x/sum(x)})}
get_entropies <- function(df){apply(df,2,function(x){-1*sum(x*log2(x))})}

letter_probabilities<-get_prob(letter_freqs[,2:74])
letter_entropies<-get_entropies(letter_probabilities)


let_pos<-c(1,1:2,1:3,1:4,1:5,1:6,1:7,1:8,1:9)
word_lengths<-c(1,rep(2,2),
               rep(3,3),
               rep(4,4),
               rep(5,5),
               rep(6,6),
               rep(7,7),
               rep(8,8),
               rep(9,9))

uncertainty_df<-data.frame(H=letter_entropies[11:(11+44)],let_pos,word_lengths)
uncertainty_df_pos1<-uncertainty_df %>%
  filter(
    let_pos == 1
  )
#######
```

```{r}
##### GET LETTER POSITION 2 - H
## read in n-gram tsv and clean up
gram_2 <- read.table('2-gram.txt',header=TRUE,sep="\t")
colnames(gram_2)<- scan(file="2-gram.txt",what="text",nlines=1,sep="\t")

  ## fix NA level
  levels(gram_2$`2-gram`)<-c(levels(gram_2$`2-gram`),as.character("NA"))
  gram_2[is.na(gram_2$`2-gram`),]$`2-gram` = as.character("NA")


  ## find and replace missing combos with 0 
  allLet<-c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z")
  allCombos<-c()
    for (i in 1:length(allLet)){
      for(j in 1:length(allLet)){
        allCombos<-c(allCombos,paste(allLet[i],allLet[j],sep=""))
      }
    }
  
  missing<-allCombos[!allCombos%in%gram_2$`2-gram`]
  missing<-cbind(missing,matrix(0,nrow = length(missing), ncol = ncol(gram_2)-1))
  colnames(missing)<-colnames(gram_2)
  gram_2<-rbind(gram_2,missing)

  ## change 0s to 1s
  gram_2[gram_2 == 0] <- 1

  #split bigrams into letter 1 & 2
  letters <- data.frame(do.call('rbind', strsplit(as.character(gram_2$`2-gram`),'',fixed=TRUE)))
  colnames(letters)<-c('n-1','n')
  names(gram_2)[names(gram_2) == '2-gram'] <- 'bigram'
  gram_2<-cbind(letters,gram_2)
  
  #remove unnecessary columns
  gram_2<-gram_2[,-4:-12]
  gram_2<-gram_2[,-40:-56]
  gram_2[,4:39]<-apply(gram_2[,4:39],2,function(x){as.numeric(x)})

#############
  
  ## GET ENTROPIES
  get_prob<- function(df) {apply(df,2,function(x){x/sum(x)})}
  get_entropies <- function(df){apply(df,2,function(x){-1*sum(x*log2(x))})}
  
  letter_probabilities<-(with(gram_2,
       by(gram_2[,4:39],gram_2[,'n-1'], get_prob,simplify= TRUE)
  ))
  
  letter_entropies<-lapply(letter_probabilities,get_entropies)
  letter_entropies<-list.rbind(letter_entropies)

  #column means
  means<-colMeans(letter_entropies)
  
  #create data frame
 let_pos<-c(2:2,2:3,2:4,2:5,2:6,2:7,2:8,2:9)
word_lengths<-c(rep(2,1),
               rep(3,2),
               rep(4,3),
               rep(5,4),
               rep(6,5),
               rep(7,6),
               rep(8,7),
               rep(9,8))

uncertainty_df_pos2<-data.frame(H=means,let_pos,word_lengths)

uncertainty_df_pos2<-uncertainty_df_pos2[uncertainty_df_pos2$let_pos == 2,]
```

```{r}
##### GET LETTER POSITION > 3 H
## read in n-gram tsv and clean up
gram_3 <- read.table('3-gram.txt',header=TRUE,sep="\t")
colnames(gram_3)<- scan(file="3-gram.txt",what="text",nlines=1,sep="\t")



  ## find and replace missing combos with 0 
  allLet<-c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z")
  allCombos<-c()
    for (i in 1:length(allLet)){
      for(j in 1:length(allLet)){
        for(k in 1:length(allLet)){
          allCombos<-c(allCombos,paste(allLet[i],paste(allLet[j],allLet[k],sep=""),sep=""))
        }
        
      }
    }
  
  missing<-allCombos[!allCombos%in%gram_3$`3-gram`]
  missing<-cbind(missing,matrix(0,nrow = length(missing), ncol = ncol(gram_3)-1))
  colnames(missing)<-colnames(gram_3)
  gram_3<-rbind(gram_3,missing)

  ## change 0s to 1s
  gram_3[gram_3 == 0] <- 1

  #split bigrams into letter 1 & 2
  letters <- data.frame(do.call('rbind', strsplit(as.character(gram_3$`3-gram`),'',fixed=TRUE)))
  colnames(letters)<-c('n-2','n-1','n')
  letters$`n-1`<-paste(letters$`n-2`,letters$`n-1`,sep="")
  letters<-letters[,-1]
  names(gram_3)[names(gram_3) == '3-gram'] <- 'trigram'
  gram_3<-cbind(letters,gram_3)
  
  #remove unnecessary columns
  gram_3<-gram_3[,-4:-11]
  gram_3<-gram_3[,-32:-47]
  gram_3[,4:31]<-apply(gram_3[,4:31],2,function(x){as.numeric(x)})

#############
  
  ## GET ENTROPIES
  get_prob<- function(df) {apply(df,2,function(x){x/sum(x)})}
  get_entropies <- function(df){apply(df,2,function(x){-1*sum(x*log2(x))})}
  
  letter_probabilities<-(with(gram_3,
       by(gram_3[,4:31],gram_3[,'n-1'], get_prob,simplify= TRUE)
  ))
  
  letter_entropies<-lapply(letter_probabilities,get_entropies)
  letter_entropies<-list.rbind(letter_entropies)

  #column means
  means<-colMeans(letter_entropies)
  
  #create data frame
 let_pos<-c(3:3,3:4,3:5,3:6,3:7,3:8,3:9)
word_lengths<-c(
               rep(3,1),
               rep(4,2),
               rep(5,3),
               rep(6,4),
               rep(7,5),
               rep(8,6),
               rep(9,7))

uncertainty_df_pos3<-data.frame(H=means,let_pos,word_lengths)
uncertainty_df_pos3<-uncertainty_df_pos3[uncertainty_df_pos3$let_pos == 3,]
```

```{r}
library(tidyr)

##### GET LETTER POSITION > 3 H
## read in n-gram tsv and clean up
gram_4 <- read.table('4-gram.txt',header=TRUE,sep="\t")
colnames(gram_4)<- scan(file="4-gram.txt",what="text",nlines=1,sep="\t")

library(gtools)
allCombos<-permutations(26, 4, LETTERS[1:26], repeats.allowed=TRUE)
allCombos<-apply( allCombos[ , 1:4] , 1 , paste , collapse = "" )

  
  missing<-allCombos[!allCombos%in%gram_4$`4-gram`]
  missing<-cbind(missing,matrix(0,nrow = length(missing), ncol = ncol(gram_4)-1))
  colnames(missing)<-colnames(gram_4)
  gram_4<-rbind(gram_4,missing)

  ## change 0s to 1s
  gram_4[gram_4 == 0] <- 1

  #split bigrams into letter 1 & 2
  letters <- data.frame(do.call('rbind', strsplit(as.character(gram_4$`4-gram`),'',fixed=TRUE)))
  colnames(letters)<-c('n-3','n-2','n-1','n')
  letters$`n-1`<-paste(letters$'n-3',paste(letters$`n-2`,letters$`n-1`,sep=""),sep="")
  letters<-letters[,-1:-2]
  names(gram_4)[names(gram_4) == '4-gram'] <- 'quadgram'
  gram_4<-cbind(letters,gram_4)
  
  #remove unnecessary columns
  gram_4<-gram_4[,-4:-10]
  gram_4<-gram_4[,-25:-45]
  gram_4[,4:24]<-apply(gram_4[,4:24],2,function(x){as.numeric(x)})

  avg<-apply(gram_4[,4:24],1,mean)
  gram_4<-cbind(gram_4,avg)
  gram_4<-gram_4[gram_4$avg > 1,]
#############
  
  ## GET ENTROPIES
  get_prob<- function(df) {apply(df,2,function(x){x/sum(x)})}
  get_entropies <- function(df){apply(df,2,function(x){-1*sum(x*log2(x))})}
  
  letter_probabilities<-(with(gram_4,
       by(gram_4[,4:24],gram_4[,'n-1'], get_prob,simplify= TRUE)
  ))
  
  letter_entropies<-lapply(letter_probabilities,get_entropies)
  letter_entropies<-list.rbind(letter_entropies)

  #column means
  means<-colMeans(letter_entropies)
  
  #create data frame
 let_pos<-c(4:4,4:5,4:6,4:7,4:8,4:9)
word_lengths<-c(
               rep(4,1),
               rep(5,2),
               rep(6,3),
               rep(7,4),
               rep(8,5),
               rep(9,6))

uncertainty_df_pos4<-data.frame(H=means,let_pos,word_lengths)



```

```{r}
uncertainty_df<-rbind(uncertainty_df_pos1,uncertainty_df_pos2,uncertainty_df_pos3,uncertainty_df_pos4)
  #gram_2_test<-merge.data.frame(gram_2,letter_entropies,by.x=('n-1'),by.y=('n-1'))

uncertainty_df$let_pos<-as.factor(uncertainty_df$let_pos)
uncertainty_df$word_lengths<-as.factor(uncertainty_df$word_lengths)

######

## graphs


ggplot(uncertainty_df,aes(x=let_pos,y=H,group=word_lengths,color=word_lengths))+
  geom_line()+
  geom_point()+
  theme_classic()+
  ggtitle("Mean Entropy (H) as a Function of Letter Position and Word Length")


ggplot(uncertainty_df,aes(x=let_pos,y=H, color=word_lengths,group=word_lengths))+
  geom_line()+
  geom_point()+
  theme_classic()+
  ggtitle("Mean Entropy (H) as a Function of Letter Position and Word Length") + 
  facet_wrap(~word_lengths)


ggplot(sum_data,aes(x=let_pos,y=mean_IKSIs,color=word_lengths,group=word_lengths))+
  geom_line()+
  geom_point()+
  theme_classic()+
  ggtitle("Mean IKSI as a Function of Letter Position and Word Length")+
  facet_wrap(~word_lengths)

total_df<-merge.data.frame(uncertainty_df,sum_data)

ggplot(total_df,aes(x=H,y=mean_IKSIs))+
  geom_smooth(method = "lm") +
  geom_point()+
  theme_classic()+
  ggtitle("Mean IKSI as a Function of Letter Position and Word Length")



```


# group level R squared

```{r}
# Analysis
# Get the means by word length and letter position for each subject
subject_means <- the_data %>%
                filter (Letters != " ",
                      !grepl("[[:punct:]]",Letters),
                      !grepl("[0-9]",Letters),
                      !grepl("[[A-Z]]*",Letters)) %>%
              group_by(Subject,word_lengths,let_pos) %>%
              summarize(mean_IKSI = mean(IKSIs, na.rm = TRUE))

# Get the grand means by averaging over subject means
sum_data <- subject_means %>%
              group_by(word_lengths,let_pos) %>%
              summarize(mean_IKSIs = mean(mean_IKSI, na.rm = TRUE),
                        SE = stde(mean_IKSI))


sum_data <- sum_data[sum_data$let_pos < 10, ]
sum_data <- sum_data[sum_data$word_lengths < 10 &
                     sum_data$word_lengths > 0, ]

sum_data$let_pos<-as.factor(sum_data$let_pos)
sum_data$word_lengths<-as.factor(sum_data$word_lengths)

group_means<-merge(sum_data,uncertainty_df, by= c("let_pos","word_lengths"))

cor.test(group_means$mean_IKSIs,group_means$H)


```

# subject level R squared

```{r}

# Analysis
# Get the means by word length and letter position for each subject
subject_means <- the_data %>%
                filter (Letters != " ",
                      !grepl("[[:punct:]]",Letters),
                      !grepl("[0-9]",Letters),
                      !grepl("[[A-Z]]*",Letters)) %>%
  
                filter(let_pos < 10,
                       word_lengths < 10,
                       word_lengths > 0) %>%
              group_by(Subject,word_lengths,let_pos) %>%
              summarize(mean_IKSI = mean(IKSIs, na.rm = TRUE))


subject_means<-merge(subject_means,uncertainty_df, by= c("let_pos","word_lengths"))

summary<-subject_means %>%
  group_by(Subject) %>%
  summarize(
    p.value = cor.test(mean_IKSI,H)$p.value,
    correlation = cor.test(mean_IKSI,H)$estimate,
    r_squared = cor.test(mean_IKSI,H)$estimate^2
  ) %>%
  ungroup() %>%
  summarize(
    mean_p.value = mean(p.value),
    mean_correlation = mean(correlation),
    mean_rsquared = mean(r_squared)
  )
  
print(summary)


```