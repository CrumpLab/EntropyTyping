---
title: "RepeatabilityModel"
author: "Walter"
date: "June 15, 2018"
output: html_document
---

SEQUENTIAL DETERMINANTS OF INFORMATION PROCESSING IN SERIAL AND DISCRETE CHOICE REACTION TIME1 by Sylvan Kornblum gave an equation for calculating repeatability
of a frequency distribution of choices with probabilities.  (Similar to H a summary statistic)   

ProbabilityNonrepetition = Sigma( Probability of J * (1-probability of J))
for each j  choice.   Probability of repetition for entire distribution =
1 - probability of nonrepetition 
see Page 114

I calculated the probability of repetition for each of the 45 letter position distributions and added the Repetition value as R to our uncertainty_df

I correlated the probability of repetition to the allsimsdf simulated retrieval times

The r squared are also pretty high around 0.7 ~ 0.8.  

One thing I noticed was that for each H condition, the repeatability was also changing,
meaning H wasn't the only variable being correlated with retrieval times.
Take a look:


```{r}
library(bit64)
library(data.table)
library(matrixStats)
# run natural language entropy sim
get_retrieval_time <- function(num_traces,monte_sim_number,rnorm_mean,rnorm_sd) {
  sampled_retrieval_times <- matrix(rnorm(num_traces*monte_sim_number,rnorm_mean,rnorm_sd),
                                    ncol=num_traces,
                                    nrow=monte_sim_number)
  min_retrieval_times <- rowMins(sampled_retrieval_times)
  return(mean(min_retrieval_times))
}


letter_freqs <- fread("C:/Users/Walt/Desktop/CognitionLab/ngrams1.csv",integer64="numeric")
letter_freqs[letter_freqs==0]<-1
letter_probabilities <- apply(letter_freqs[,2:46],2,function(x){x/sum(x)})


all_sims_df <- data.frame()

position <-c(1,1:2,1:3,1:4,1:5,1:6,1:7,1:8,1:9)
word_length <-c(1,rep(2,2),
                rep(3,3),
                rep(4,4),
                rep(5,5),
                rep(6,6),
                rep(7,7),
                rep(8,8),
                rep(9,9))

for (l in 1:45){
  letter_probs <- letter_probabilities[,l]
  amount_of_practice <- c(50,100,200,500)
  
  mean_letter_retrieval_time <- length(length(amount_of_practice))
  for (i in 1:length(amount_of_practice)){
    letter_trace_frequencies <- floor(letter_probs*amount_of_practice[i])
    letter_trace_frequencies[letter_trace_frequencies==0] <- 1 # for convenience, we always assume there is 1 trace
    letter_retrieval_times     <- unlist(lapply(letter_trace_frequencies,
                                                function(x) {get_retrieval_time(x,100,500,100)}))
    # reset letter_trace_frequencies to include zeros for computing grand_mean
    letter_trace_frequencies <- floor(letter_probs*amount_of_practice[i])
    mean_letter_retrieval_time[i] <- sum(letter_retrieval_times*letter_trace_frequencies)/sum(letter_trace_frequencies)
  }
  
  sim_df_natural <- data.frame(amount_of_practice,
                               position = position[l],
                               word_length = word_length[l],
                               mean_letter_retrieval_time)
  all_sims_df <- rbind(all_sims_df,sim_df_natural)
}

all_sims_df$position<-as.factor(all_sims_df$position)
all_sims_df$word_length<-as.factor(all_sims_df$word_length)



# from Matt's analysis let's compute H for letter uncertainty across position and word length

# load in the excel file from Norvig:
letter_freqs <- fread("C:/Users/Walt/Desktop/CognitionLab/ngrams1.csv",integer64="numeric")
letter_freqs[letter_freqs==0]<-1

letter_probabilities <- apply(letter_freqs[,2:46],2,function(x){x/sum(x)})

letter_entropies <- apply(letter_probabilities,2,function(x){-1*sum(x*log2(x))})
letter_repeatability<- apply(letter_probabilities,2,function(x){sum(x*(1-x))})
position<-c(1,1:2,1:3,1:4,1:5,1:6,1:7,1:8,1:9)
word_length<-c(1,rep(2,2),
               rep(3,3),
               rep(4,4),
               rep(5,5),
               rep(6,6),
               rep(7,7),
               rep(8,8),
               rep(9,9))

uncertainty_df<-data.frame(R=letter_repeatability,H=letter_entropies,position,word_length)

cor(all_sims_df[all_sims_df$amount_of_practice==50,]$mean_letter_retrieval_time,
    uncertainty_df$H)^2



cor(all_sims_df[all_sims_df$amount_of_practice==100,]$mean_letter_retrieval_time,
    uncertainty_df$H)^2

cor(all_sims_df[all_sims_df$amount_of_practice==200,]$mean_letter_retrieval_time,
    uncertainty_df$H)^2

cor(all_sims_df[all_sims_df$amount_of_practice==500,]$mean_letter_retrieval_time,
    uncertainty_df$H)^2


cor(all_sims_df[all_sims_df$amount_of_practice==50,]$mean_letter_retrieval_time,
    uncertainty_df$R)^2
cor(all_sims_df[all_sims_df$amount_of_practice==100,]$mean_letter_retrieval_time,
    uncertainty_df$R)^2
cor(all_sims_df[all_sims_df$amount_of_practice==200,]$mean_letter_retrieval_time,
    uncertainty_df$R)^2
cor(all_sims_df[all_sims_df$amount_of_practice==500,]$mean_letter_retrieval_time,
    uncertainty_df$R)^2

```